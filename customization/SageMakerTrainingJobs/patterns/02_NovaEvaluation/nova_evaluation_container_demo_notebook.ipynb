{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova evaluation container: Feature examples\n",
    "\n",
    "This notebook shows how to use the Nova evaluation container to evaluate large language models. You'll work through a support ticket classification example that demonstrates key features for production model evaluation.\n",
    "|\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook covers four main areas:\n",
    "\n",
    "- **Metadata passthrough**: Keep custom fields (like difficulty level, domain, priority) with your data during evaluation. This lets you see how your model performs on different types of inputs without extra data processing.\n",
    "- **BYOM (Bring your own metrics)**: Use Lambda functions to validate outputs against your specific requirements. Instead of just checking accuracy, you can verify JSON formatting, business rules, and domain-specific constraints.\n",
    "- **Confidence & Failure Analysis with logprobs**: Extract token-level probabilities to understand when your model is uncertain. This helps identify overconfident predictions and calibration issues.\n",
    "- **Multi-Node evalutaion**: Run evaluations across multiple instances to handle large datasets efficiently.\n",
    "\n",
    "## The Example\n",
    "\n",
    "You'll evaluate a support ticket classifier using 40 examples with different categories, difficulty levels, and business domains. The notebook shows how to:\n",
    "\n",
    "- Set up custom Lambda functions for validation\n",
    "- Configure evaluation recipes with log probabilities\n",
    "- Launch distributed evaluation jobs on SageMaker\n",
    "- Analyze results to find specific failure patterns\n",
    "\n",
    "By the end, you'll understand how to move beyond simple accuracy scores to get detailed insights about where and why your model fails. This approach helps you make \n",
    "targeted improvements rather than guessing what needs to be fixed.\n",
    "\n",
    "The techniques work for any text classification task, but the principles apply to other model types as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- AWS account with IAM permissions for Lambda, SageMaker, and S3\n",
    "- SageMaker training instance: `ml.g5.12xlarge` (single or multi-node)\n",
    "- Nova custom evaluation SDK layer (https://github.com/aws/nova-custom-eval-sdk/releases)\n",
    "- Nova evaluation container: `708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-HP-Eval-latest` (updated 10/23/25)\n",
    "\n",
    "This notebook runs locally or in SageMaker Studio. For BYOM workflows, you'll need the custom evaluation layer published to Lambda.\n",
    "\n",
    "**Note:** This notebook uses the updated evaluation container (10/23/25) with improved logprobs:\n",
    "- Fixed logprobs count (previously missing ~27% of entries)\n",
    "- Corrected top logprob values (no longer overwritten)\n",
    "- Column renamed from `pred_logits` to `pred_logprobs`\n",
    "- Tokens now include SentencePiece prefix characters (▁) for better context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install SageMaker SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker\n",
    "import sagemaker\n",
    "print(sagemaker.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import boto3\n",
    "# SageMaker imports\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation with Metadata Passthrough\n",
    "\n",
    "Metadata passthrough preserves custom fields during evaluation, letting you analyze performance by data segments without additional processing steps.\n",
    "\n",
    "Standard evaluation gives you overall metrics like 95% accuracy. This doesn't show performance differences across customer types, product categories, or other important dimensions.Without metadata passthrough, you need separate tracking systems and complex data joins after evaluation. This creates extra work and potential data consistency issues.\n",
    "\n",
    "With metadata passthrough, you can see performance breakdowns immediately. For example, you might find 98% accuracy on billing queries but 60% accuracy on technical support cases.This detailed view helps identify specific problems. You can see patterns like \"the model fails on complex logistics cases\" or \"confidence scores are unreliable for legalcompliance queries.\" This makes it easier to focus improvements on the right areas.\n",
    "\n",
    "Metadata passthrough shows you not just whether your model works, but where it works well and where it doesn't. Here we take advantage of this feature to pass metadata fields `category`, `difficulty`, `domain`, and  `priority`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_dataset(file_path: str = 'text_eval_dataset.jsonl') -> List[Dict]:\n",
    "    \"\"\"Load the sample dataset with metadata\"\"\"\n",
    "    dataset = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            dataset.append(json.loads(line))\n",
    "    return dataset\n",
    "\n",
    "def analyze_dataset_metadata(dataset: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Analyze metadata distribution in the dataset\"\"\"\n",
    "    metadata_list = []\n",
    "    for item in dataset:\n",
    "        metadata = json.loads(item.get('metadata', '{}'))\n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    return pd.DataFrame(metadata_list)\n",
    "\n",
    "# Load and analyze the sample dataset\n",
    "dataset = load_sample_dataset()\n",
    "metadata_df = analyze_dataset_metadata(dataset)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"\\nMetadata fields: {list(metadata_df.columns)}\")\n",
    "print(f\"\\nMetadata distribution:\")\n",
    "for col in metadata_df.columns:\n",
    "    print(f\"{col}: {metadata_df[col].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics with BYOM Workflow\n",
    "\n",
    "Without custom metrics, you evaluate models in two separate steps: check general performance, then validate production requirements. This creates a gap where models pass evaluation but fail in production due to format errors or business rule violations.\n",
    "\n",
    "Bring-your-own-metrics (BYOM) puts your production validation logic into the evaluation pipeline. Instead of relying on accuracy scores alone, you can check that responses follow JSON schemas, meet business constraints, and handle edge cases correctly.\n",
    "\n",
    "This helps because production failures often come from format issues, not content problems. A model might generate accurate responses that break JSON parsing or miss required fields. Custom metrics catch these problems during evaluation.\n",
    "\n",
    "The benefit is alignment between evaluation and production. Your evaluation metrics match your production validation logic, so test results better predict real-world performance. BYOM help ensure your model works correctly in your actual system, not just on academic benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda function code for custom metrics (BYOM)\n",
    "lambda_code = '''\n",
    "import json\n",
    "from nova_custom_evaluation_sdk.processors.decorators import preprocess, postprocess\n",
    "from nova_custom_evaluation_sdk.lambda_handler import build_lambda_handler\n",
    "\n",
    "@preprocess\n",
    "def preprocessor(event: dict, context) -> dict:\n",
    "    data = event.get('data', {})\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": {\n",
    "            \"system\": data.get(\"system\"),\n",
    "            \"prompt\": data.get(\"prompt\", \"\"),\n",
    "            \"gold\": data.get(\"gold\", \"\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "@postprocess\n",
    "def postprocessor(event: dict, context) -> dict:\n",
    "    data = event.get('data', {})\n",
    "    inference_output = data.get('inference_output', '')\n",
    "    gold = data.get('gold', '')\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    # 1. Schema Validation\n",
    "    schema_valid = validate_schema(inference_output)\n",
    "    metrics.append({\"metric\": \"schema_validation\", \"value\": 1.0 if schema_valid else 0.0})\n",
    "    \n",
    "    # 2. Classification Metrics\n",
    "    class_metrics = calculate_class_metrics(inference_output, gold)\n",
    "    metrics.extend(class_metrics)\n",
    "    \n",
    "    return {\"statusCode\": 200, \"body\": metrics}\n",
    "\n",
    "def validate_schema(output: str) -> bool:\n",
    "    try:\n",
    "        parsed = json.loads(output)\n",
    "        return \"class\" in parsed\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def calculate_class_metrics(inference_output: str, gold: str) -> list:\n",
    "    pred_class = extract_class(inference_output)\n",
    "    true_class = extract_class(gold)\n",
    "    \n",
    "    if not pred_class or not true_class:\n",
    "        return [{\"metric\": \"class_accuracy\", \"value\": 0.0}]\n",
    "    \n",
    "    accuracy = 1.0 if pred_class.lower() == true_class.lower() else 0.0\n",
    "    return [{\"metric\": \"class_accuracy\", \"value\": accuracy}]\n",
    "\n",
    "def extract_class(text: str) -> str:\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        return str(parsed.get(\"class\", \"\")).strip()\n",
    "    except:\n",
    "        return text.strip()\n",
    "\n",
    "lambda_handler = build_lambda_handler(\n",
    "    preprocessor=preprocessor,\n",
    "    postprocessor=postprocessor\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Custom Lambda function code for BYOM workflow:\")\n",
    "print(\"- Schema validation metric\")\n",
    "print(\"- Classification accuracy metric\")\n",
    "print(\"- Preprocessing and postprocessing hooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use this code and create a lambda function with the added layer as shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Function Setup\n",
    "\n",
    "Rather than embedding custom logic directly in evaluation scripts, the Nova container calls Lambda functions for metric computation. This separation enables version control of evaluation logic, independent scaling of compute resources, and reuse across different evaluation jobs.\n",
    "\n",
    "The setup involves three components: publishing the Nova evaluation SDK as a Lambda layer, creating an execution role, and deploying the function with custom metrics code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Lambda Layer\n",
    "\n",
    "The Nova custom evaluation SDK provides decorators and utilities for metric computation. Publishing it as a Lambda layer allows reuse across multiple evaluation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "layer_name = \"nova-custom-eval-layer\"\n",
    "zip_file_path = 'nova-custom-eval-layer.zip'\n",
    "\n",
    "# Get region with fallback\n",
    "region = os.environ.get('AWS_DEFAULT_REGION') or boto3.Session().region_name or 'us-east-1'\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "\n",
    "if os.path.exists(zip_file_path):\n",
    "    try:\n",
    "        with open(zip_file_path, 'rb') as f:\n",
    "            response = lambda_client.publish_layer_version(\n",
    "                LayerName=layer_name,\n",
    "                Content={'ZipFile': f.read()},\n",
    "                CompatibleRuntimes=['python3.9', 'python3.10', 'python3.11', 'python3.12']\n",
    "            )\n",
    "        layer_arn = response['LayerVersionArn']\n",
    "        print(f\"Published layer: {layer_arn}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    # Check for existing layer\n",
    "    try:\n",
    "        response = lambda_client.list_layer_versions(LayerName=layer_name, MaxItems=1)\n",
    "        layer_arn = response['LayerVersions'][0]['LayerVersionArn']\n",
    "        print(f\"Using existing layer: {layer_arn}\")\n",
    "    except:\n",
    "        raise Exception(f\"Layer zip not found. Download from https://github.com/aws/nova-custom-eval-sdk/releases\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IAM Execution Role\n",
    "\n",
    "Lambda functions need an execution role to run. This code checks for an existing role and creates one if needed, with a trust policy allowing the Lambda service to assume it and basic execution permissions for CloudWatch logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# Get region with fallback\n",
    "region = os.environ.get('AWS_DEFAULT_REGION') or boto3.Session().region_name or 'us-east-1'\n",
    "\n",
    "iam_client = boto3.client(\"iam\", region_name=region)\n",
    "lambda_client = boto3.client(\"lambda\", region_name=region)\n",
    "sts_client = boto3.client(\"sts\", region_name=region)\n",
    "\n",
    "role_name = \"nova-custom-eval-lambda-role\"\n",
    "\n",
    "try:\n",
    "    account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "    lambda_client.list_functions(MaxItems=1)\n",
    "    \n",
    "    try:\n",
    "        role_response = iam_client.get_role(RoleName=role_name)\n",
    "        role_arn = role_response[\"Role\"][\"Arn\"]\n",
    "        print(f\"Using existing role: {role_arn}\")\n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        trust_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        role_response = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "            Description=\"Execution role for Nova custom evaluation Lambda\"\n",
    "        )\n",
    "        \n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "        )\n",
    "        \n",
    "        role_arn = role_response[\"Role\"][\"Arn\"]\n",
    "        print(f\"Created role: {role_arn}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Lambda Function\n",
    "\n",
    "This code packages the custom metrics code into a deployment zip, creates or updates the Lambda function, and attaches the evaluation layer. The function ARN will be used in the evaluation recipe to call our custom metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "import time\n",
    "\n",
    "function_name = \"nova-custom-eval\"\n",
    "region = os.environ.get('AWS_DEFAULT_REGION') or boto3.Session().region_name or 'us-east-1'\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    lambda_file = os.path.join(tmp_dir, \"lambda_function.py\")\n",
    "    with open(lambda_file, \"w\") as f:\n",
    "        f.write(lambda_code)\n",
    "    \n",
    "    zip_path = os.path.join(tmp_dir, \"function.zip\")\n",
    "    with zipfile.ZipFile(zip_path, \"w\") as z:\n",
    "        z.write(lambda_file, arcname=\"lambda_function.py\")\n",
    "    \n",
    "    with open(zip_path, \"rb\") as f:\n",
    "        zip_content = f.read()\n",
    "    \n",
    "    lambda_client = boto3.client(\"lambda\", region_name=region)\n",
    "    \n",
    "    try:\n",
    "        lambda_client.get_function(FunctionName=function_name)\n",
    "        print(f\"Updating function {function_name}...\")\n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=function_name,\n",
    "            ZipFile=zip_content\n",
    "        )\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        print(f\"Creating function {function_name}...\")\n",
    "        response = lambda_client.create_function(\n",
    "            FunctionName=function_name,\n",
    "            Runtime=\"python3.12\",\n",
    "            Role=role_arn,\n",
    "            Handler=\"lambda_function.lambda_handler\",\n",
    "            Code={\"ZipFile\": zip_content},\n",
    "            Timeout=30,\n",
    "            MemorySize=256\n",
    "        )\n",
    "    \n",
    "    # Wait for function to be ready\n",
    "    while True:\n",
    "        config = lambda_client.get_function_configuration(FunctionName=function_name)\n",
    "        if config[\"LastUpdateStatus\"] == \"Successful\":\n",
    "            break\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Update layers - include both custom eval layer and Powertools layer\n",
    "    # Powertools layer provides Pydantic for structured validation of metric outputs\n",
    "    powertools_layer_arn = f\"arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPythonV3-python312-arm64:1\"\n",
    "    \n",
    "    response = lambda_client.update_function_configuration(\n",
    "        FunctionName=function_name,\n",
    "        Runtime=\"python3.12\",  # Update runtime for existing functions\n",
    "        Layers=[layer_arn, powertools_layer_arn]\n",
    "    )\n",
    "    \n",
    "    function_arn = response[\"FunctionArn\"]\n",
    "    print(f\"Lambda Function ARN: {function_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Evaluation Recipe\n",
    "\n",
    "\n",
    "The recipe YAML defines the complete evaluation pipeline: model selection, inference parameters, and processing configuration. Key settings include `top_logprobs: 10` for confidence analysis and the Lambda ARN for custom metrics processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "recipe_config = {\n",
    "    \"run\": {\n",
    "        \"name\": \"support-ticket-classification\",\n",
    "        \"model_type\": \"amazon.nova-lite-v1:0:300k\",\n",
    "        \"model_name_or_path\": \"nova-lite/prod\",\n",
    "        \"replicas\": 1\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"task\": \"gen_qa\",\n",
    "        \"strategy\": \"gen_qa\",\n",
    "        \"metric\": \"all\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_logprobs\": 10\n",
    "    },\n",
    "    \"processor\": {\n",
    "        \"lambda_arn\": function_arn,\n",
    "        \"preprocessing\": {\"enabled\": True},\n",
    "        \"postprocessing\": {\"enabled\": True},\n",
    "        \"aggregation\": \"average\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"lambda_recipe.yaml\", \"w\") as f:\n",
    "    yaml.dump(recipe_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"Recipe saved with Lambda ARN: {function_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch SageMaker Evaluation Job\n",
    "\n",
    "SageMaker provides the compute infrastructure and container orchestration for Nova evaluations. The training job abstraction handles resource provisioning, data loading, and result collection. Critical configuration includes the Nova container image, compute instance type, and S3 paths for input data and output results.\n",
    "\n",
    "The training job configuration specifies the Nova container image, instance type, and S3 locations. Key parameters include the evaluation recipe path, input dataset location, and output directory for results. Update the `input_s3_uri`, `output_s3_uri`, and `role` fields to match your AWS environment before execution. Ensure your dataset is uploaded to the S3 location specified in `input_s3_uri`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Use your actual SageMaker execution role ARN\n",
    "role = \"arn:aws:iam::975050356504:role/service-role/AmazonSageMaker-ExecutionRole-20240404T210962\"\n",
    "\n",
    "input_s3_uri = \"s3://kb-doc-bucket-2025/Input/text_eval_dataset.jsonl\"\n",
    "output_s3_uri = \"s3://kb-doc-bucket-2025/output/\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    output_path=output_s3_uri,\n",
    "    base_job_name=\"support-ticket-classification\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    training_recipe=\"lambda_recipe.yaml\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=\"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-HP-Eval-latest\"\n",
    "    \n",
    ")\n",
    "\n",
    "print(f\"Using role: {role}\")\n",
    "estimator.fit(inputs={\"train\": TrainingInput(s3_data=input_s3_uri)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation runtime depends on dataset size, model complexity, and instance type. The 40-example dataset typically completes in 20-40 minutes on `ml.g5.12xlarge` instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Evaluation Results\n",
    "\n",
    "The evaluation outputs include comprehensive metrics, individual predictions, and token-level log probabilities stored in the S3 output directory. These results enable detailed performance analysis and confidence-based failure investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a parquet file into a pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the parquet file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The loaded DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download, extract, and load evaluation results from S3, including detailed metrics, predictions, and log probabilities for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "def download_and_extract_output(output_s3_uri, job_name, output_dir='./evaluation_results'):\n",
    "    \"\"\"\n",
    "    Download output.tar.gz from S3 and extract it to a local directory.\n",
    "    Returns the path to the parquet results file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if output_s3_uri.startswith('s3://'):\n",
    "        output_s3_uri = output_s3_uri[5:]\n",
    "    \n",
    "    if output_s3_uri.endswith('/'):\n",
    "        output_s3_uri = output_s3_uri[:-1]\n",
    "    \n",
    "    parts = output_s3_uri.split('/')\n",
    "    bucket = parts[0]\n",
    "    prefix = '/'.join(parts[1:])\n",
    "    \n",
    "    s3_key = f\"{prefix}/{job_name}/output/output.tar.gz\"\n",
    "    local_tar_path = os.path.join(output_dir, \"output.tar.gz\")\n",
    "    \n",
    "    print(f\"Downloading from s3://{bucket}/{s3_key} to {local_tar_path}\")\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        s3_client.download_file(bucket, s3_key, local_tar_path)\n",
    "        print(\"Download completed successfully\")\n",
    "        \n",
    "        print(f\"Extracting to {output_dir}\")\n",
    "        with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=output_dir)\n",
    "        print(\"Extraction completed successfully\")\n",
    "        \n",
    "        os.remove(local_tar_path)\n",
    "        print(f\"Removed {local_tar_path}\")\n",
    "        \n",
    "        # Find the parquet file\n",
    "        parquet_pattern = os.path.join(output_dir, \"**\", \"*.parquet\")\n",
    "        parquet_files = glob.glob(parquet_pattern, recursive=True)\n",
    "        \n",
    "        if parquet_files:\n",
    "            results_path = parquet_files[0]\n",
    "            print(f\"\\nResults parquet file: {results_path}\")\n",
    "            return results_path\n",
    "        else:\n",
    "            print(\"Warning: No parquet file found\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the job name from the estimator you just ran\n",
    "job_name = estimator.latest_training_job.name\n",
    "print(f\"Latest job name: {job_name}\")\n",
    "\n",
    "# Download the results - returns path to parquet file\n",
    "parquet_path = download_and_extract_output(\n",
    "    output_s3_uri=output_s3_uri,\n",
    "    job_name=job_name,\n",
    "    output_dir='./evaluation_results_new'\n",
    ")\n",
    "\n",
    "# Load the parquet file directly\n",
    "if parquet_path:\n",
    "    results_df = pd.read_parquet(parquet_path)\n",
    "    print(f\"Loaded {len(results_df)} rows from results\")\n",
    "    print(\"Columns:\", results_df.columns.tolist())\n",
    "else:\n",
    "    print(\"Failed to load results - no parquet file found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Results Structure\n",
    "\n",
    "The Nova container outputs structured results to the S3 path specified in the recipe configuration. The output directory contains evaluation metrics, detailed predictions, and TensorBoard logs organized in the following hierarchy:\n",
    "\n",
    "\n",
    "```\n",
    "job_name/\n",
    "├── eval-results/\n",
    "│    └── results_[timestamp].json\n",
    "│    └── inference_output.jsonl (only present for gen_qa)\n",
    "│    └── details/\n",
    "│        └── model/\n",
    "│            └── execution-date-time/\n",
    "│                └──details_task_name_#_datetime.parquet\n",
    "└── tensorboard-results/\n",
    "    └── eval/\n",
    "        └── events.out.tfevents.[timestamp]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Analysis with Log Probabilities\n",
    "\n",
    "This section implements confidence analysis with raw model prediction logits into actionable insights. We examine the relationship between model confidence and prediction quality to identify failure patterns that could compromise model trustworthiness. For our classifcation use case we can simply look at the first token of the classification string as our classes do not all start with the same tokens. This eliminates the complexity of combining tokens and probabilities then calculating a mean to be used in downstream scoring.\n",
    "\n",
    "\n",
    "The analysis covers:\n",
    "\n",
    "- **Confidence extraction**: Convert prediction logits into confidence scores\n",
    "- **Failure detection**: Identify overconfident or uncertain predictions  \n",
    "- **Visualization**: Create charts to show model reliability patterns\n",
    "\n",
    "Components:\n",
    "\n",
    "- **Logit processing**: Parse prediction data and handle different formats\n",
    "- **Token analysis**: Extract prediction tokens and calculate probabilities\n",
    "- **Failure classification**: Find low-confidence predictions and overconfident errors\n",
    "\n",
    "This analysis shows:\n",
    "- How to extract confidence metrics from model outputs\n",
    "- Which patterns indicate model failures\n",
    "- How to visualize confidence data\n",
    "- When confidence scores match actual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "\n",
    "def calculate_confidence_score(pred_logprobs_str, prediction_text=\"\") -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate confidence score from log probabilities for the predicted class.\n",
    "    \n",
    "    Args:\n",
    "        pred_logprobs_str (str): String representation of prediction logprobs (updated field name as of 10/23/25)\n",
    "        prediction_text (str): The actual prediction text to extract class from\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (confidence, predicted_token, filtered_keys) - \n",
    "               confidence: The first token probability of the predicted class\n",
    "               predicted_token: The predicted class name\n",
    "               filtered_keys: List of tokens in the class name\n",
    "    \n",
    "    Note: Tokens may include SentencePiece prefix characters (▁) as of container update 10/23/25\n",
    "    \"\"\"\n",
    "    if not pred_logprobs_str or pred_logprobs_str == '[]':\n",
    "        return 0.0, \"\", []\n",
    "    \n",
    "    try:\n",
    "        # Extract the predicted class from the prediction text\n",
    "        predicted_class = \"\"\n",
    "        if prediction_text:\n",
    "            try:\n",
    "                parsed = json.loads(prediction_text)\n",
    "                predicted_class = str(parsed.get(\"class\", \"\")).strip()\n",
    "            except:\n",
    "                predicted_class = \"\"\n",
    "        \n",
    "        if not predicted_class:\n",
    "            return 0.0, \"\", []\n",
    "        \n",
    "        logprobs = ast.literal_eval(pred_logprobs_str)\n",
    "        if not logprobs or len(logprobs) == 0:\n",
    "            return 0.0, predicted_class, []\n",
    "        \n",
    "        # Build dictionary of all tokens and their probabilities\n",
    "        token_probs = {}\n",
    "        for token_prob_dict in logprobs[0]:\n",
    "            if isinstance(token_prob_dict, dict) and token_prob_dict:\n",
    "                max_key = max(token_prob_dict.items(), key=lambda x: x[1])[0]\n",
    "                max_logprob = token_prob_dict[max_key]\n",
    "                token_probs[max_key] = np.exp(max_logprob)\n",
    "        \n",
    "        # Find tokens that match the predicted class\n",
    "        class_tokens = []\n",
    "        class_probs = []\n",
    "        \n",
    "        # Split class name into potential tokens\n",
    "        class_parts = re.split(r'[_\\s]+', predicted_class)\n",
    "        \n",
    "        for token, prob in token_probs.items():\n",
    "            # Strip SentencePiece prefix (▁) and other special characters for matching\n",
    "            clean_token = re.sub(r'[^a-zA-Z0-9]', '', token)\n",
    "            if clean_token:\n",
    "                # Check if this token matches any part of the class name\n",
    "                for part in class_parts:\n",
    "                    if part and (clean_token.lower() == part.lower() or \n",
    "                                part.lower().startswith(clean_token.lower()) or\n",
    "                                clean_token.lower().startswith(part.lower())):\n",
    "                        class_tokens.append(clean_token)\n",
    "                        class_probs.append(prob)\n",
    "                        break\n",
    "        \n",
    "        # Use first token confidence (OpenAI's classification approach)\n",
    "        if class_probs:\n",
    "            confidence = float(class_probs[0])\n",
    "        else:\n",
    "            confidence = 0.0\n",
    "        \n",
    "        return confidence, predicted_class, class_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing log probs: {e}\")\n",
    "        return 0.0, \"\", []\n",
    "\n",
    "\n",
    "def analyze_low_confidence_failures(results_df: pd.DataFrame, confidence_threshold: float = 0.7, quality_threshold: float = 0.3) -> dict:\n",
    "    \"\"\"Perform comprehensive failure analysis\"\"\"\n",
    "    \n",
    "    # Make a copy to avoid modifying original\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Helper to extract prediction text\n",
    "    def get_prediction_text(row):\n",
    "        pred = row['predictions']\n",
    "        \n",
    "        # Handle string representation of array: \"['text']\" \n",
    "        if isinstance(pred, str):\n",
    "            try:\n",
    "                pred_list = ast.literal_eval(pred)\n",
    "                if isinstance(pred_list, list) and len(pred_list) > 0:\n",
    "                    return str(pred_list[0]).strip()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Handle actual array\n",
    "        if isinstance(pred, (list, np.ndarray)) and len(pred) > 0:\n",
    "            return str(pred[0]).strip()\n",
    "        \n",
    "        return str(pred).strip()\n",
    "    \n",
    "    # Apply the function with both logprobs and prediction text\n",
    "    confidence_results = df.apply(\n",
    "        lambda row: calculate_confidence_score(row['pred_logprobs'], get_prediction_text(row)), \n",
    "        axis=1\n",
    "    )\n",
    "    df[['confidence', 'predicted_token', 'filtered_keys']] = pd.DataFrame(\n",
    "        list(confidence_results), \n",
    "        index=df.index,\n",
    "        columns=['confidence', 'predicted_token', 'filtered_keys']\n",
    "    )\n",
    "    \n",
    "    df['quality_score'] = df['metrics'].apply(\n",
    "        lambda x: ast.literal_eval(x).get('f1', 0) if x else 0\n",
    "    )\n",
    "    df['is_correct'] = df['quality_score'] >= quality_threshold\n",
    "    \n",
    "    # Analyze low confidence predictions\n",
    "    low_conf_df = df[df['confidence'] < confidence_threshold]\n",
    "    \n",
    "    # Also identify high confidence but low quality (overconfident errors)\n",
    "    overconfident_df = df[(df['confidence'] >= confidence_threshold) & \n",
    "                          (df['quality_score'] < quality_threshold)]\n",
    "    \n",
    "    analysis_results = {\n",
    "        'summary': {\n",
    "            'total_predictions': len(df),\n",
    "            'low_confidence_count': len(low_conf_df),\n",
    "            'low_confidence_rate': len(low_conf_df) / len(df) if len(df) > 0 else 0,\n",
    "            'overconfident_errors': len(overconfident_df),\n",
    "            'overconfident_rate': len(overconfident_df) / len(df) if len(df) > 0 else 0,\n",
    "            'avg_confidence': df['confidence'].mean(),\n",
    "            'avg_quality': df['quality_score'].mean(),\n",
    "            'overall_accuracy': df['is_correct'].mean()\n",
    "        },\n",
    "        'low_confidence_examples': [],\n",
    "        'overconfident_examples': []\n",
    "    }\n",
    "    \n",
    "    # Low confidence examples\n",
    "    if len(low_conf_df) > 0:\n",
    "        for idx, row in low_conf_df.iterrows():\n",
    "            try:\n",
    "                specifics = ast.literal_eval(row['specifics'])\n",
    "                metadata = json.loads(specifics.get('metadata', '{}'))\n",
    "                \n",
    "                analysis_results['low_confidence_examples'].append({\n",
    "                    'example': row['example'][:100] + '...' if len(row['example']) > 100 else row['example'],\n",
    "                    'confidence': row['confidence'],\n",
    "                    'quality_score': row['quality_score'],\n",
    "                    'category': metadata.get('category', 'unknown'),\n",
    "                    'difficulty': metadata.get('difficulty', 'unknown'),\n",
    "                    'domain': metadata.get('domain', 'unknown')\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Overconfident errors\n",
    "    if len(overconfident_df) > 0:\n",
    "        for idx, row in overconfident_df.iterrows():\n",
    "            try:\n",
    "                specifics = ast.literal_eval(row['specifics'])\n",
    "                metadata = json.loads(specifics.get('metadata', '{}'))\n",
    "                \n",
    "                analysis_results['overconfident_examples'].append({\n",
    "                    'example': row['example'][:100] + '...' if len(row['example']) > 100 else row['example'],\n",
    "                    'confidence': row['confidence'],\n",
    "                    'quality_score': row['quality_score'],\n",
    "                    'category': metadata.get('category', 'unknown'),\n",
    "                    'difficulty': metadata.get('difficulty', 'unknown'),\n",
    "                    'domain': metadata.get('domain', 'unknown')\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "# Use already loaded data\n",
    "analysis_results = analyze_low_confidence_failures(results_df, confidence_threshold=0.7, quality_threshold=0.3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal predictions: {analysis_results['summary']['total_predictions']}\")\n",
    "print(f\"Average confidence: {analysis_results['summary']['avg_confidence']:.3f}\")\n",
    "print(f\"Average F1 quality: {analysis_results['summary']['avg_quality']:.3f}\")\n",
    "print(f\"Overall accuracy (F1>0.3): {analysis_results['summary']['overall_accuracy']:.1%}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"LOW CONFIDENCE PREDICTIONS (confidence < 0.7)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Count: {analysis_results['summary']['low_confidence_count']} ({analysis_results['summary']['low_confidence_rate']:.1%})\")\n",
    "for i, example in enumerate(analysis_results['low_confidence_examples'][:5], 1):\n",
    "    print(f\"\\n{i}. Confidence: {example['confidence']:.3f} | F1: {example['quality_score']:.3f}\")\n",
    "    print(f\"   {example['difficulty']} | {example['domain']}\")\n",
    "    print(f\"   {example['example']}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"OVERCONFIDENT ERRORS (confidence >= 0.7 but F1 < 0.3)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Count: {analysis_results['summary']['overconfident_errors']} ({analysis_results['summary']['overconfident_rate']:.1%})\")\n",
    "for i, example in enumerate(analysis_results['overconfident_examples'][:5], 1):\n",
    "    print(f\"\\n{i}. Confidence: {example['confidence']:.3f} | F1: {example['quality_score']:.3f}\")\n",
    "    print(f\"   {example['difficulty']} | {example['domain']}\")\n",
    "    print(f\"   {example['example']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Result Parsing\n",
    "\n",
    "Model evaluation systems produce messy data. Predictions come wrapped in JSON strings, nested arrays, or Python literals depending on which framework generated them. Before we can analyze confidence patterns, we need to extract the actual predictions and probability scores from these varied formats.\n",
    "\n",
    "Our parsing functions handle three main tasks:\n",
    "\n",
    "- **Data Format Conversion**: Transform JSON strings, Python literals, and nested arrays into consistent structures for analysis.\n",
    "- **Information Extraction**: Pull class predictions, confidence scores, and metadata from evaluation results.\n",
    "- **Error Recovery**: Deal with malformed data, missing fields, and parsing failures that happen in real evaluation pipelines.\n",
    "\n",
    "What looks like simple parsing actually solves a practical problem: evaluation systems rarely produce data in the same format twice. Different models, frameworks, and even individual predictions can generate outputs in completely different structures. Our functions need to handle this variation while extracting consistent data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_gold(text):\n",
    "    \"\"\"\n",
    "    More robust extraction handling various formats and bytes\n",
    "    \"\"\"\n",
    "    is_bytes = isinstance(text, bytes)\n",
    "    \n",
    "    if is_bytes:\n",
    "        patterns = [\n",
    "            rb'\"class\"\\s*:\\s*\"([^\"]+)\"',\n",
    "            rb'\"class\"\\s*:\\s*([^,}\\s]+)',\n",
    "            rb\"'class'\\s*:\\s*'([^']+)'\",\n",
    "            rb\"'class'\\s*:\\s*([^,}\\s]+)\",\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                value = match.group(1).decode('utf-8', errors='ignore').strip()\n",
    "                value = re.sub(r'[.,;]+$', '', value)\n",
    "                return value\n",
    "    else:\n",
    "        patterns = [\n",
    "            r'\"class\"\\s*:\\s*\"([^\"]+)\"',\n",
    "            r'\"class\"\\s*:\\s*([^,}\\s]+)',\n",
    "            r\"'class'\\s*:\\s*'([^']+)'\",\n",
    "            r\"'class'\\s*:\\s*([^,}\\s]+)\",\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                value = match.group(1).strip()\n",
    "                value = re.sub(r'[.,;]+$', '', value)\n",
    "                return value\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_preds(text):\n",
    "    \"\"\"\n",
    "    Parse a malformed JSON string into a dictionary\n",
    "    \"\"\"\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    text = text.strip()\n",
    "    if text.startswith('[') and text.endswith(']'):\n",
    "        text = text[1:-1].strip()\n",
    "    \n",
    "    if (text.startswith(\"'\") and text.endswith(\"'\")) or \\\n",
    "       (text.startswith('\"') and text.endswith('\"')):\n",
    "        text = text[1:-1]\n",
    "    \n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.replace('\\\\\"', '\"')\n",
    "    \n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        result = {}\n",
    "        \n",
    "        class_match = re.search(r'\"class\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "        if class_match:\n",
    "            result['class'] = class_match.group(1)\n",
    "        \n",
    "        thought_match = re.search(r'\"thought\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "        if thought_match:\n",
    "            result['thought'] = thought_match.group(1)\n",
    "        \n",
    "        desc_match = re.search(r'\"description\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "        if desc_match:\n",
    "            result['description'] = desc_match.group(1)\n",
    "        \n",
    "        return result if result else None\n",
    "\n",
    "# Success confirmation\n",
    "print(\"✓ Helper functions defined successfully:\")\n",
    "print(\"  - extract_gold(): Extracts class from gold standard\")\n",
    "print(\"  - parse_preds(): Parses prediction text to dict\")\n",
    "print(\"\\nFunctions are ready to use for data processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions transform raw log probability strings into meaningful confidence scores. The `parse_log_probs` function extracts token-level probability information from the model's output, handling the nested dictionary format that contains each token's likelihood.\n",
    "\n",
    "The `find_target_tokens` function addresses a key challenge: class names like \"Software_Bug\" might be tokenized as multiple pieces. We need to find all possible token representations to accurately calculate confidence scores.\n",
    "\n",
    "The `extract_class_confidence` function brings everything together, searching through the probability sequence to find tokens matching our target class. When multiple tokens are found, it uses geometric mean to combine their probabilities—a more statistically sound approach than simple multiplication. This gives us reliable confidence scores that reflect the model's actual uncertainty about its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def parse_log_probs(data_str):\n",
    "    \"\"\"Parse the log probability data string into a list of token dictionaries\"\"\"\n",
    "    pattern = r\"\\{([^}]+)\\}\"\n",
    "    matches = re.findall(pattern, data_str)\n",
    "    \n",
    "    parsed_sequence = []\n",
    "    for match in matches:\n",
    "        pairs = re.findall(r\"'([^']+)':\\s*([-\\d.e]+)\", match)\n",
    "        token_dict = {token: float(log_prob) for token, log_prob in pairs}\n",
    "        parsed_sequence.append(token_dict)\n",
    "    \n",
    "    return parsed_sequence\n",
    "\n",
    "def find_target_tokens(target_class):\n",
    "    \"\"\"Split target class into component tokens\"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    if '_' in target_class:\n",
    "        parts = target_class.split('_')\n",
    "        for i, part in enumerate(parts):\n",
    "            if i > 0:\n",
    "                tokens.append('_')\n",
    "            subtokens = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z][a-z]|\\b)', part)\n",
    "            tokens.extend(subtokens)\n",
    "    else:\n",
    "        tokens = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z][a-z]|\\b)', target_class)\n",
    "    \n",
    "    return [target_class] + tokens\n",
    "\n",
    "def extract_class_confidence(data_str, target_class, use_first_occurrence=True):\n",
    "    \"\"\"Extract confidence score for target class prediction.\n",
    "    \n",
    "    Args:\n",
    "        data_str: Log probability data string\n",
    "        target_class: Target class to find\n",
    "        use_first_occurrence: If True, only use first match of each token (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with confidence metrics using geometric mean for multi-token sequences\n",
    "    \"\"\"\n",
    "    parsed_sequence = parse_log_probs(data_str)\n",
    "    possible_tokens = find_target_tokens(target_class)\n",
    "    \n",
    "    print(f\"Searching for target class: '{target_class}'\")\n",
    "    print(f\"Possible token sequences: {possible_tokens}\\n\")\n",
    "    \n",
    "    # Track which tokens we've found and at what positions\n",
    "    found_tokens = []\n",
    "    used_positions = set()\n",
    "    \n",
    "    # Search for tokens in order, using first occurrence only\n",
    "    for token in possible_tokens:\n",
    "        for step_idx, token_dict in enumerate(parsed_sequence):\n",
    "            if use_first_occurrence and step_idx in used_positions:\n",
    "                continue\n",
    "            if token in token_dict:\n",
    "                log_prob = token_dict[token]\n",
    "                prob = math.exp(log_prob)\n",
    "                found_tokens.append({\n",
    "                    'step': step_idx,\n",
    "                    'token': token,\n",
    "                    'log_prob': log_prob,\n",
    "                    'probability': prob,\n",
    "                    'confidence_pct': prob * 100\n",
    "                })\n",
    "                used_positions.add(step_idx)\n",
    "                break  # Use first occurrence of this token\n",
    "    \n",
    "    if not found_tokens:\n",
    "        print(f\"❌ Target class '{target_class}' not found in sequence\")\n",
    "        return None\n",
    "    \n",
    "    if len(found_tokens) == 1:\n",
    "        result = found_tokens[0]\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"✓ FOUND: '{target_class}' as single token\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Step: {result['step']}\")\n",
    "        print(f\"Log Probability: {result['log_prob']:.10f}\")\n",
    "        print(f\"Confidence: {result['confidence_pct']:.6f}%\")\n",
    "        print(\"=\" * 70)\n",
    "        return result\n",
    "    else:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"✓ FOUND: '{target_class}' as token sequence\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Use geometric mean (via log space) instead of multiplying probabilities\n",
    "        log_probs = [t['log_prob'] for t in found_tokens]\n",
    "        mean_log_prob = np.mean(log_probs)\n",
    "        overall_prob = np.exp(mean_log_prob)\n",
    "        \n",
    "        for token_info in found_tokens:\n",
    "            print(f\"  Step {token_info['step']:2d} | '{token_info['token']:12s}' | \"\n",
    "                  f\"Confidence: {token_info['confidence_pct']:8.4f}%\")\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "        print(f\"Geometric Mean Confidence: {overall_prob*100:.6f}%\")\n",
    "        print(f\"Mean Log Probability: {mean_log_prob:.10f}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return {\n",
    "            'tokens': found_tokens,\n",
    "            'overall_probability': overall_prob,\n",
    "            'overall_log_prob': mean_log_prob,\n",
    "            'confidence_pct': overall_prob * 100\n",
    "        }\n",
    "\n",
    "def parse_to_dict(text):\n",
    "    \"\"\"Parse a malformed JSON string into a dictionary\"\"\"\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    original_text = text\n",
    "    text = text.strip()\n",
    "    \n",
    "    if text.startswith('[') and text.endswith(']'):\n",
    "        text = text[1:-1].strip()\n",
    "    \n",
    "    if (text.startswith(\"'\") and text.endswith(\"'\")) or \\\n",
    "       (text.startswith('\"') and text.endswith('\"')):\n",
    "        text = text[1:-1]\n",
    "    \n",
    "    try:\n",
    "        result = ast.literal_eval(original_text)\n",
    "        if isinstance(result, dict):\n",
    "            for key, value in result.items():\n",
    "                if isinstance(value, str) and (value.strip().startswith('{') or value.strip().startswith('[')):\n",
    "                    try:\n",
    "                        result[key] = json.loads(value)\n",
    "                    except:\n",
    "                        pass\n",
    "            return result\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    \n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.replace('\\\\\"', '\"')\n",
    "    text = text.replace(\"\\\\'\", \"'\")\n",
    "    \n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    result = {}\n",
    "    patterns = [\n",
    "        r'\"(\\w+)\"\\s*:\\s*\"([^\"]*)\"',\n",
    "        r\"'(\\w+)'\\s*:\\s*'([^']*)'\",\n",
    "        r'\"(\\w+)\"\\s*:\\s*({[^}]+})',\n",
    "        r\"'(\\w+)'\\s*:\\s*({[^}]+})\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        for key, value in matches:\n",
    "            if value.strip().startswith('{'):\n",
    "                try:\n",
    "                    result[key] = json.loads(value.replace(\"'\", '\"'))\n",
    "                except:\n",
    "                    result[key] = value\n",
    "            else:\n",
    "                result[key] = value\n",
    "    \n",
    "    return result if result else None\n",
    "\n",
    "# Success confirmation\n",
    "print(\"✓ Log probability and parsing functions defined successfully:\")\n",
    "print(\"  - parse_log_probs(): Parses log probability strings\")\n",
    "print(\"  - find_target_tokens(): Tokenizes target class names\")\n",
    "print(\"  - extract_class_confidence(): Extracts confidence for predictions\")\n",
    "print(\"  - parse_to_dict(): Parses malformed JSON/dict strings\")\n",
    "print(\"\\nAll functions are ready for confidence analysis and data processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This function handles the final step in our data extraction pipeline: pulling the actual class predictions from various output formats. Unlike the previous functions that dealt with raw parsing, `extract_class_value` focuses specifically on finding class labels regardless of how they're wrapped or formatted.\n",
    "\n",
    "The function demonstrates the messy reality of model outputs. Predictions might come as:\n",
    "- String representations of Python lists: `\"['Document_Request']\"`\n",
    "- NumPy arrays from batch processing  \n",
    "- Clean JSON dictionaries: `{\"class\": \"Document_Request\"}`\n",
    "- Markdown-formatted text: `**Issue Type:** Document Request`\n",
    "- Raw strings with various quote styles\n",
    "\n",
    "Each format requires a different extraction strategy. The function tries the most structured approaches first (JSON parsing), then falls back to regex patterns for less structured text. This cascading approach ensures we can extract class labels from virtually any output format while maintaining accuracy.\n",
    "\n",
    "The test section validates our extraction logic against real evaluation data, showing how the same function handles both prediction and gold standard formats. This verification step is crucial—if we can't reliably extract class labels, our entire accuracy analysis becomes meaningless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_class_value(text):\n",
    "    \"\"\"Extract class value from various text formats\"\"\"\n",
    "    # Handle string representation of list: \"['...']\"\n",
    "    if isinstance(text, str) and text.startswith('['):\n",
    "        try:\n",
    "            parsed_list = ast.literal_eval(text)\n",
    "            if isinstance(parsed_list, list) and len(parsed_list) > 0:\n",
    "                text = parsed_list[0]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Handle numpy array\n",
    "    if isinstance(text, np.ndarray) and len(text) > 0:\n",
    "        text = text[0]\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Try JSON format: {\"class\": \"Document_Request\"}\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        if isinstance(parsed, dict) and 'class' in parsed:\n",
    "            return parsed['class']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try markdown format: **Issue Type:** Document Request\n",
    "    patterns = [\n",
    "        r'\\*\\*Issue Type:\\*\\*\\s*([^\\n]+)',\n",
    "        r'Issue Type:\\s*([^\\n]+)',\n",
    "        r'\"class\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        r\"'class'\\s*:\\s*'([^']+)'\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = match.group(1).strip()\n",
    "            # Clean up common suffixes\n",
    "            value = re.sub(r'\\s*/\\s*.*$', '', value)  # Remove \"/ Department\" part\n",
    "            return value\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test it\n",
    "print(\"Testing extraction:\")\n",
    "pred_sample = results_df['predictions'].iloc[0]\n",
    "gold_sample = results_df['gold'].iloc[0]\n",
    "\n",
    "print(f\"Prediction extracted: {extract_class_value(pred_sample)}\")\n",
    "print(f\"Gold extracted: {extract_class_value(gold_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Preview the analysis results showing predictions, gold labels, correctness, confidence scores, and metadata for the first few samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Node Scaling (optional)\n",
    "\n",
    "Optionally you can scale evaluation to larger datasets using **multi-node evaluation** by simply increasing the replica count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-node configuration - just change the replicas parameter\n",
    "multinode_config = {\n",
    "    \"run\": {\n",
    "        \"name\": \"support-ticket-classification-multinode\",\n",
    "        \"model_name_or_path\": \"nova-lite-v1:0\",\n",
    "        \"replicas\": 4  # Scale to 4 nodes automatically you must have quota allocation for this\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"task\": \"gen_qa\",\n",
    "        \"strategy\": \"gen_qa\",\n",
    "        \"metric\": \"all\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_logprobs\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Multi-node scaling configuration:\")\n",
    "print(f\"Replicas: {multinode_config['run']['replicas']}\")\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"- Automatic workload distribution\")\n",
    "print(\"- Preserved metadata-based analysis\")\n",
    "print(\"- Deterministic result aggregation\")\n",
    "print(\"- Scale from thousands to millions of examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization and Performance Analysis\n",
    "\n",
    "Moving beyond aggregate metrics requires systematic analysis of model behavior patterns. This comprehensive visualization framework transforms raw evaluation data into actionable intelligence by examining performance through multiple analytical lenses.\n",
    "\n",
    "Let's xamine four critical performance dimensions:\n",
    "\n",
    "**Confidence Calibration Analysis**: Model confidence scores reveal calibration quality - well-calibrated models show strong correlation between confidence and accuracy. Poor calibration indicates overconfident predictions on incorrect outputs, suggesting need for confidence penalty training or temperature scaling.\n",
    "\n",
    "**Domain-Specific Performance Patterns**: Different domains (IT support, billing, security) exhibit varying complexity levels and linguistic patterns. Performance gaps across domains indicate training data imbalances or domain-specific feature requirements.\n",
    "\n",
    "**Difficulty-Accuracy Relationships**: The relationship between labeled difficulty and model performance reveals whether the model struggles with inherently complex cases or exhibits systematic biases independent of true difficulty.\n",
    "\n",
    "**Metadata Cross-Tabulation**: Examining performance at metadata intersections (e.g., hard IT support tickets vs. easy billing queries) identifies compound failure modes where multiple factors interact to degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Helper function to get prediction text\n",
    "def get_prediction_text(row):\n",
    "    pred = row['predictions']\n",
    "    if isinstance(pred, str):\n",
    "        try:\n",
    "            pred_list = ast.literal_eval(pred)\n",
    "            if isinstance(pred_list, list) and len(pred_list) > 0:\n",
    "                return str(pred_list[0]).strip()\n",
    "        except:\n",
    "            pass\n",
    "    if isinstance(pred, (list, np.ndarray)) and len(pred) > 0:\n",
    "        return str(pred[0]).strip()\n",
    "    return str(pred).strip()\n",
    "\n",
    "# Check if confidence already exists, if not calculate it\n",
    "if 'confidence' not in results_df.columns:\n",
    "    # calculate_confidence_score returns (confidence, predicted_token, filtered_keys)\n",
    "    # Extract only the confidence value (first element)\n",
    "    results_df['confidence'] = results_df.apply(\n",
    "        lambda row: calculate_confidence_score(row['pred_logprobs'], get_prediction_text(row))[0],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "print(\"Confidence calculated!\")\n",
    "print(\"Mean confidence:\", results_df['confidence'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "# Extract metadata\n",
    "def extract_metadata(row):\n",
    "    try:\n",
    "        specifics = ast.literal_eval(row['specifics'])\n",
    "        metadata = json.loads(specifics.get('metadata', '{}'))\n",
    "        return pd.Series({\n",
    "            'meta_category': metadata.get('category', 'unknown'),\n",
    "            'meta_difficulty': metadata.get('difficulty', 'unknown'),\n",
    "            'meta_domain': metadata.get('domain', 'unknown')\n",
    "        })\n",
    "    except:\n",
    "        return pd.Series({'meta_category': 'unknown', 'meta_difficulty': 'unknown', 'meta_domain': 'unknown'})\n",
    "\n",
    "# Add missing columns\n",
    "results_df['pred_class'] = results_df['predictions'].apply(extract_class_value)\n",
    "results_df['gold_class'] = results_df['gold'].apply(extract_class_value)\n",
    "results_df['correct'] = results_df['pred_class'] == results_df['gold_class']\n",
    "results_df[['meta_category', 'meta_difficulty', 'meta_domain']] = results_df.apply(extract_metadata, axis=1)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].hist(results_df['confidence'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axvline(0.7, color='red', linestyle='--', label='Low confidence threshold')\n",
    "axes[0, 0].set_title('Confidence Score Distribution')\n",
    "axes[0, 0].set_xlabel('Confidence Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "confidence_bins = pd.cut(results_df['confidence'], bins=5)\n",
    "accuracy_by_conf = results_df.groupby(confidence_bins, observed=True)['correct'].mean()\n",
    "accuracy_by_conf.plot(kind='bar', ax=axes[0, 1], rot=45)\n",
    "axes[0, 1].set_title('Accuracy by Confidence Bin')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "\n",
    "domain_perf = results_df.groupby('meta_domain')['correct'].mean()\n",
    "domain_perf.plot(kind='bar', ax=axes[1, 0], color='skyblue')\n",
    "axes[1, 0].set_title('Accuracy by Domain')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "difficulty_perf = results_df.groupby('meta_difficulty')['correct'].mean()\n",
    "difficulty_perf.plot(kind='bar', ax=axes[1, 1], color='lightcoral')\n",
    "axes[1, 1].set_title('Accuracy by Difficulty')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Analysis and Cross-Tabulation Intelligence\n",
    "\n",
    "\n",
    "Aggregate metrics mask critical performance patterns that determine real-world model reliability. This systematic stratified analysis framework dissects model behavior across metadata dimensions, revealing interaction effects and failure modes invisible to traditional evaluation approaches.\n",
    "\n",
    "The analysis progresses through three complementary diagnostic layers: **stratified confidence analysis** identifies calibration failures across data segments, **cross-tabulation analysis** reveals interaction effects between confidence and metadata dimensions, and **problematic segment identification** prioritizes intervention targets based on failure severity.\n",
    "\n",
    "This multi-dimensional approach transforms evaluation from reactive performance measurement into proactive failure mode detection, enabling precise, targeted model improvements rather than broad optimization efforts. By systematically examining confidence-accuracy relationships across domain, difficulty, and category intersections, teams can identify exactly where models fail and why, converting scattered performance issues into structured improvement roadmaps.\n",
    "\n",
    "The framework addresses three critical evaluation limitations: **calibration assessment** across heterogeneous data segments, **interaction effect detection** between model confidence and task characteristics, and **intervention prioritization** based on failure severity and statistical significance. This systematic approach enables data-driven optimization strategies that maximize improvement impact while minimizing resource expenditure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Distribution by Metadata\n",
    "\n",
    "The stratified confidence analysis performs systematic model calibration assessment across metadata dimensions. This diagnostic framework segments evaluation results by difficulty, domain, and category, calculating three critical metrics for each segment:\n",
    "\n",
    "- **Average confidence score** - Quantifies model certainty about predictions\n",
    "- **Sample count** - Ensures statistical significance of findings  \n",
    "- **Accuracy rate** - Measures actual prediction correctness\n",
    "\n",
    "Well-calibrated models exhibit consistent confidence-accuracy relationships across all data segments. Significant deviations reveal systematic biases, training data imbalances, or calibration failures that aggregate metrics cannot detect.\n",
    "\n",
    "**Calibration Assessment**: Models should demonstrate proportional confidence-accuracy alignment. High confidence predictions should achieve higher accuracy rates, with this relationship holding consistently across different data segments.\n",
    "\n",
    "**Failure Mode Identification**: Segments showing high confidence but low accuracy represent dangerous overconfident predictions that could mislead users or compromise downstream system reliability.\n",
    "\n",
    "**Resource Allocation Intelligence**: Low-performing segments with sufficient sample sizes indicate precisely where targeted improvements (data augmentation, domain-specific fine-tuning) would yield maximum impact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONFIDENCE DISTRIBUTION BY METADATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Confidence by difficulty\n",
    "print(\"\\n=== Confidence by Difficulty ===\")\n",
    "for difficulty in ['easy', 'medium', 'hard']:\n",
    "    subset = results_df[results_df['meta_difficulty'] == difficulty]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"{difficulty:8s}: avg confidence = {subset['confidence'].mean():.3f}, \"\n",
    "              f\"count = {len(subset):3d}, accuracy = {subset['correct'].mean():.1%}\")\n",
    "\n",
    "# Confidence by domain\n",
    "print(\"\\n=== Confidence by Domain ===\")\n",
    "for domain in sorted(results_df['meta_domain'].unique()):\n",
    "    subset = results_df[results_df['meta_domain'] == domain]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"{domain:15s}: avg confidence = {subset['confidence'].mean():.3f}, \"\n",
    "              f\"count = {len(subset):3d}, accuracy = {subset['correct'].mean():.1%}\")\n",
    "\n",
    "# Confidence by category\n",
    "print(\"\\n=== Confidence by Category ===\")\n",
    "for category in sorted(results_df['meta_category'].unique()):\n",
    "    subset = results_df[results_df['meta_category'] == category]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"{category:20s}: avg confidence = {subset['confidence'].mean():.3f}, \"\n",
    "              f\"count = {len(subset):3d}, accuracy = {subset['correct'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Diagnostic Patterns**\n",
    "\n",
    "The analysis reveals four critical calibration patterns:\n",
    "\n",
    "1. **Confidence-Accuracy Misalignment**: High confidence combined with low accuracy indicates dangerous overconfidence requiring temperature scaling or confidence penalty training\n",
    "2. **Conservative Calibration**: Low confidence with high accuracy suggests overly conservative models that may benefit from confidence boosting techniques\n",
    "3. **Training Data Quality Issues**: Large accuracy gaps between domains indicate unbalanced training data requiring targeted augmentation\n",
    "4. **Systematic Bias Detection**: Consistent underperformance in specific segments reveals model limitations requiring architectural or training strategy adjustments\n",
    "\n",
    "This approach transforms aggregate performance metrics into precise diagnostic intelligence, enabling targeted model improvements rather than broad, unfocused optimization efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Tabulation Analysis\n",
    "\n",
    "The cross-tabulation analysis reveals interaction effects between model confidence and metadata dimensions, identifying compound failure modes where multiple factors interact to degrade performance. This approach bins confidence scores into interpretable categories (Low: 0-0.3, Medium: 0.3-0.7, High: 0.7-1.0) and examines accuracy patterns across these intersections.\n",
    "\n",
    "Cross-tabulation moves beyond simple averages into multidimensional performance maps, revealing how confidence calibration varies across different data characteristics. The analysis generates two critical outputs for each intersection:\n",
    "\n",
    "- **Accuracy matrices** - Show actual performance at confidence-metadata intersections\n",
    "- **Sample count matrices** - Ensure statistical significance of observed patterns\n",
    "\n",
    "**Why it Matters**\n",
    "\n",
    "**Interaction Effect Detection**: Simple stratified analysis might miss cases where confidence calibration breaks down only under specific conditions (e.g., high-confidence predictions in certain domains showing unexpectedly low accuracy).\n",
    "\n",
    "**Calibration Validation**: Well-calibrated models should show consistent accuracy improvements as confidence increases, regardless of metadata characteristics. Deviations indicate systematic calibration failures.\n",
    "\n",
    "**Compound Failure Identification**: Some failure modes only emerge when multiple factors combine (e.g., medium-confidence predictions in hard cases performing worse than expected).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CROSS-TABULATION: CONFIDENCE × CORRECTNESS × METADATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create confidence bins\n",
    "results_df['conf_bin'] = pd.cut(results_df['confidence'], \n",
    "                                 bins=[0, 0.3, 0.7, 1.0], \n",
    "                                 labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Cross-tab: Confidence vs Difficulty (showing accuracy)\n",
    "print(\"\\n=== Accuracy by Confidence Level × Difficulty ===\")\n",
    "crosstab_diff = pd.crosstab(results_df['conf_bin'], \n",
    "                             results_df['meta_difficulty'],\n",
    "                             values=results_df['correct'],\n",
    "                             aggfunc='mean')\n",
    "print(crosstab_diff.fillna('-').round(3))\n",
    "\n",
    "# Show counts for context\n",
    "print(\"\\n=== Sample Count by Confidence Level × Difficulty ===\")\n",
    "count_tab_diff = pd.crosstab(results_df['conf_bin'], \n",
    "                              results_df['meta_difficulty'])\n",
    "print(count_tab_diff)\n",
    "\n",
    "# Cross-tab: Confidence vs Domain (showing accuracy)\n",
    "print(\"\\n=== Accuracy by Confidence Level × Domain (Top Domains) ===\")\n",
    "# Only show domains with at least 3 samples\n",
    "domain_counts = results_df['meta_domain'].value_counts()\n",
    "top_domains = domain_counts[domain_counts >= 3].index.tolist()\n",
    "\n",
    "if top_domains:\n",
    "    filtered_df = results_df[results_df['meta_domain'].isin(top_domains)]\n",
    "    crosstab_domain = pd.crosstab(filtered_df['conf_bin'], \n",
    "                                   filtered_df['meta_domain'],\n",
    "                                   values=filtered_df['correct'],\n",
    "                                   aggfunc='mean')\n",
    "    print(crosstab_domain.fillna('-').round(3))\n",
    "    \n",
    "    print(\"\\n=== Sample Count by Confidence Level × Domain (Top Domains) ===\")\n",
    "    count_tab_domain = pd.crosstab(filtered_df['conf_bin'], \n",
    "                                    filtered_df['meta_domain'])\n",
    "    print(count_tab_domain)\n",
    "else:\n",
    "    print(\"Not enough samples per domain for meaningful cross-tabulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights**\n",
    "\n",
    "The cross-tabulation reveals four critical interaction patterns:\n",
    "\n",
    "1. **Confidence-Domain Interactions**: Domains where high confidence doesn't correlate with high accuracy indicate domain-specific overconfidence requiring targeted calibration\n",
    "2. **Difficulty-Confidence Misalignment**: Cases where confidence patterns don't match difficulty levels suggest either labeling inconsistencies or model biases\n",
    "3. **Sparse Cell Analysis**: Empty or low-count cells reveal data distribution issues and potential blind spots in model training\n",
    "4. **Calibration Consistency**: Systematic deviations from expected confidence-accuracy relationships across metadata dimensions indicate fundamental calibration problems\n",
    "\n",
    "**Actionable Intelligence from Cross-Tabulation**\n",
    "\n",
    "This analysis enables precise intervention targeting:\n",
    "- **Domain-Specific Calibration**: Apply temperature scaling selectively to domains showing confidence-accuracy misalignment\n",
    "- **Training Data Rebalancing**: Address sparse cells through targeted data collection\n",
    "- **Architecture Modifications**: Consider domain-aware confidence estimation for consistently miscalibrated segments\n",
    "- **Evaluation Protocol Refinement**: Identify metadata combinations requiring additional validation\n",
    "\n",
    "Cross-tabulation changes evaluation from univariate analysis into systematic interaction detection, revealing failure modes invisible to aggregate metrics or simple stratification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problematic Segment Identification \n",
    "\n",
    "The problematic segment analysis identifies the most critical failure modes by aggregating performance across domain-difficulty intersections. This targeted approach filters for segments exhibiting both low confidence (<0.5) and low accuracy (<0.3) - the most dangerous combination indicating fundamental model failures.\n",
    "\n",
    "The code performs multi-dimensional aggregation across domain and difficulty metadata, calculating three key metrics for each intersection:\n",
    "\n",
    "- **Average confidence** - Model certainty levels for the segment\n",
    "- **Accuracy rate** - Actual performance within the segment  \n",
    "- **Sample count** - Statistical significance and intervention priority\n",
    "\n",
    "**Why Problematic Segment Detection Matters**\n",
    "\n",
    "**Critical Failure Identification**: Segments with both low confidence and low accuracy represent the most severe model failures - cases where the model both performs poorly and lacks awareness of its limitations.\n",
    "\n",
    "**Intervention Prioritization**: Limited resources require strategic allocation. This analysis identifies precisely which domain-difficulty combinations need immediate attention based on failure severity and sample size.\n",
    "\n",
    "**Risk Assessment**: Low confidence + low accuracy segments pose the highest risk in production systems, as they represent unpredictable model behavior in critical scenarios.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROBLEMATIC SEGMENTS (Low Confidence + Low Accuracy)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate by domain and difficulty\n",
    "segment_analysis = results_df.groupby(['meta_domain', 'meta_difficulty']).agg({\n",
    "    'confidence': 'mean',\n",
    "    'correct': 'mean',\n",
    "    'example': 'count'\n",
    "}).round(3)\n",
    "\n",
    "segment_analysis.columns = ['avg_confidence', 'accuracy', 'count']\n",
    "\n",
    "# Filter for problematic segments\n",
    "problematic = segment_analysis[\n",
    "    (segment_analysis['avg_confidence'] < 0.5) & \n",
    "    (segment_analysis['accuracy'] < 0.3)\n",
    "]\n",
    "\n",
    "if len(problematic) > 0:\n",
    "    print(\"\\nSegments needing attention:\")\n",
    "    print(problematic.sort_values('accuracy'))\n",
    "    \n",
    "    print(\"\\n=== Recommendations ===\")\n",
    "    for (domain, difficulty), row in problematic.iterrows():\n",
    "        print(f\"\\n{domain} / {difficulty}:\")\n",
    "        print(f\"  - {row['count']} examples with {row['accuracy']:.1%} accuracy\")\n",
    "        print(f\"  - Average confidence: {row['avg_confidence']:.3f}\")\n",
    "        print(f\"  - Action: Add more training data or improve prompts for this segment\")\n",
    "else:\n",
    "    print(\"\\nNo severely problematic segments found.\")\n",
    "    print(\"All segments have either reasonable confidence or accuracy.\")\n",
    "\n",
    "# Show all segments for comparison\n",
    "print(\"\\n=== All Segments Overview ===\")\n",
    "print(segment_analysis.sort_values('accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategic Diagnostics**\n",
    "\n",
    "The analysis reveals four intervention categories:\n",
    "\n",
    "1. **Critical Segments** (Low confidence + Low accuracy): Immediate intervention required through targeted data augmentation or prompt engineering\n",
    "2. **Training Gaps** (High confidence + Low accuracy): Overconfident failures requiring calibration and additional training data\n",
    "3. **Conservative Performance** (Low confidence + High accuracy): Acceptable but could benefit from confidence boosting\n",
    "4. **Optimal Segments** (High confidence + High accuracy): Well-functioning areas requiring no immediate intervention\n",
    "\n",
    "**Actionable Intervention Strategies**\n",
    "\n",
    "For each problematic segment, the analysis provides specific recommendations:\n",
    "\n",
    "- **Data Augmentation**: Focus collection efforts on underrepresented domain-difficulty combinations\n",
    "- **Prompt Engineering**: Refine instructions for specific challenging scenarios\n",
    "- **Architecture Modifications**: Consider specialized components for consistently problematic segments\n",
    "- **Evaluation Protocol Enhancement**: Implement additional validation for high-risk combinations\n",
    "\n",
    "**Informed Improvement Planning**\n",
    "\n",
    "The problematic segment analysis converts reactive debugging into proactive optimization, enabling systematic model improvement through targeted, data-driven interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key features shown\n",
    "\n",
    "**Metadata Passthrough**: End-to-end preservation of custom fields enables stratified analysis across domains, difficulty levels, and categories without post-hoc data joins.\n",
    "\n",
    "**Log Probabilities**: Token-level uncertainty quantification supports confidence calibration, quality gates, and hallucination detection in production systems.\n",
    "\n",
    "**Custom Metrics (BYOM)**: Complete evaluation pipeline control with schema validation, domain-specific metrics, and flexible pre/post-processing hooks.\n",
    "\n",
    "**Multi-Node Scaling**: Distributed evaluation maintains deterministic results while scaling from thousands to millions of examples through simple replica configuration.\n",
    "\n",
    "### **Data into Action**\n",
    "\n",
    "The multi-dimensional analysis approach—combining confidence calibration assessment, cross-tabulation interaction detection, and problematic segment identification—converts scattered performance issues into structured improvement roadmaps:\n",
    "\n",
    "- **Precision Targeting**: Identify exactly which domain-difficulty combinations require intervention\n",
    "- **Resource Optimization**: Prioritize improvements based on failure severity and statistical significance\n",
    "- **Risk Assessment**: Detect overconfident predictions that pose production reliability risks\n",
    "- **Systematic Improvement**: Transform reactive debugging into proactive optimization planning\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "1. **Deploy Evaluation Pipeline**: Configure your dataset and custom metrics using the Nova Container framework\n",
    "2. **Implement Stratified Analysis**: Apply confidence-metadata segmentation to identify hidden failure modes\n",
    "3. **Establish Production Monitoring**: Track calibration quality across metadata dimensions in live systems\n",
    "4. **Scale Systematically**: Use multi-node configuration for comprehensive dataset evaluation\n",
    "5. **Optimize Strategically**: Focus interventions on problematic segments with maximum impact potential\\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
