{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling Evaluation Results Analysis\n",
    "\n",
    "This notebook analyzes the Berkeley Function Calling Leaderboard (BFCL) evaluation results for different Nova models across specific test categories.\n",
    "\n",
    "## Overview\n",
    "- **Models**: custom-nova-lite, nova-lite-v1.0, nova-micro-v1.0, nova-premier-v1.0, nova-pro-v1.0\n",
    "- **Test Categories**: irrelevance, multiple, live_relevance, simple\n",
    "- **Analysis Focus**: Model-specific performance comparison using accuracy scores from individual test categories\n",
    "\n",
    "## Key Features\n",
    "- âœ… **Model-specific analysis**: Results from each model's score directory\n",
    "- âœ… **Category-focused**: Only the 4 test categories with generated results\n",
    "- âœ… **Accuracy metrics**: Using first row summary statistics from each score file\n",
    "- âœ… **Comparative visualizations**: Performance comparison across models and categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up BFCL Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r custom_model_deployment_arn_tool_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this example depends on a specific version of the framework\n",
    "!git clone --branch v1.3 https://github.com/ShishirPatil/gorilla.git bfcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd bfcl/berkeley-function-call-leaderboard\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here is the value for your model_id\", custom_model_deployment_arn_tool_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update BFCL framework\n",
    "We'll update the BFCL framework to use your custom model deployment by copying a few updated files to the correct locations. This will update the correct classes in the framework so your model can be used to generate responses using the BFCL generate and evaluate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update model_config\n",
    "Update [`model_config.py`](./model_config.py) with your custom model deployment ARN as found here. This assumes we are using nova-lite as the base model: \n",
    "\n",
    "```python    \n",
    "\"custom-nova-lite\": ModelConfig(\n",
    "        model_name=\"<CMD or PT ARN here\",  \n",
    "        display_name=\"My Custom Nova Lite Model (FC)\",\n",
    "        url=\"https://my-organization.com/models/custom-nova\",\n",
    "        org=\"My Organization\",\n",
    "        license=\"Custom License\",\n",
    "        model_handler=NovaHandler,\n",
    "        input_price=1.5,\n",
    "        output_price=6.0,\n",
    "        is_fc_model=True,\n",
    "        underscore_to_dot=True,\n",
    "        base_model=\"nova-lite-v1.0\",  # Specify the base model this custom model is derived from\n",
    "    ),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy the `model_config`, `supported_models.py`, and `nova.py` to the correct directory in the BFCL eval framework. These are modifications we need to make to support custom model evaluation on bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp model_config bfcl/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py\n",
    "!cp supported_models.py bfcl/berkeley-function-call-leaderboard/bfcl_eval/constants/supported_models.py\n",
    "!cp nova.py bfcl/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/nova.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Responses\n",
    "Uncomment each of the following commands to run the same test for our custom model and the other Nova base models for comparison.\n",
    "Before we do that, let's copy our `test_case_ids_to_generate.json` file to the correct location so we can leverage the `--run-ids` flag that will only generate responses for the evaluation data we held out of our training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp test_case_ids_to_generate.json bfcl/berkeley-function-call-leaderboard/test_case_ids_to_generate.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run our evaluation on our evaluation set using the `generate` command. If you didn't catch the model name assigned to your model, checkout [`model_config.py`](bfcl/berkeley-function-call-leaderboard/bfcl_eval/constants/model_config.py) and it should be the first model in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bfcl generate --model custom-nova-lite --run-ids\n",
    "# !bfcl generate --model nova-lite-v1.0 --run-ids\n",
    "# !bfcl generate --model nova-pro-v1.0 --run-ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluations\n",
    "With your responses generated we can now run the evaluations. This will make a folder of scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bfcl evaluate --model custom-nova-lite --test-category irrelevance,multiple,live_relevance,simple\n",
    "# !bfcl evaluate --model nova-lite-v1.0 --test-category irrelevance,multiple,live_relevance,simple\n",
    "# !bfcl evaluate --model nova-pro-v1.0 --test-category irrelevance,multiple,live_relevance,simple\n",
    "# !bfcl evaluate --model nova-premier-v1.0 --test-category irrelevance,multiple,live_relevance,simple\n",
    "# !bfcl evaluate --model nova-micro-v1.0 --test-category irrelevance,multiple,live_relevance,simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Data Ready for Analysis\n",
    "Now let's take a look at how our customized model compares to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotly for jupyter\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Load Model-Specific Score Data\n",
    "\n",
    "We'll load the accuracy scores from each model's score files. Each score file contains a summary row with accuracy, correct_count, and total_count metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_score_summary(file_path):\n",
    "    \"\"\"Load the first row (summary statistics) from a score file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            if first_line:\n",
    "                return json.loads(first_line)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def build_score_dataframe(score_folder='score'):\n",
    "    \"\"\"Build a comprehensive DataFrame from all score files.\"\"\"\n",
    "    score_data = []\n",
    "    \n",
    "    # Define the test categories we're interested in\n",
    "    test_categories = ['irrelevance', 'live_relevance', 'multiple', 'simple']\n",
    "    \n",
    "    # Iterate through model directories\n",
    "    for model_dir in os.listdir(score_folder):\n",
    "        model_path = os.path.join(score_folder, model_dir)\n",
    "        \n",
    "        # Skip if not a directory or if it's a CSV file\n",
    "        if not os.path.isdir(model_path) or model_dir.startswith('data_'):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing model: {model_dir}\")\n",
    "        \n",
    "        # Process each test category\n",
    "        for category in test_categories:\n",
    "            score_file = f\"BFCL_v3_{category}_score.json\"\n",
    "            score_path = os.path.join(model_path, score_file)\n",
    "            \n",
    "            if os.path.exists(score_path):\n",
    "                summary = load_score_summary(score_path)\n",
    "                if summary:\n",
    "                    row = {\n",
    "                        'model': model_dir,\n",
    "                        'test_category': category,\n",
    "                        'accuracy': summary.get('accuracy', 0),\n",
    "                        'correct_count': summary.get('correct_count', 0),\n",
    "                        'total_count': summary.get('total_count', 0),\n",
    "                        'accuracy_percentage': summary.get('accuracy', 0) * 100\n",
    "                    }\n",
    "                    score_data.append(row)\n",
    "                    print(f\"  {category}: {summary.get('accuracy', 0):.3f} ({summary.get('correct_count', 0)}/{summary.get('total_count', 0)})\")\n",
    "            else:\n",
    "                print(f\"  {category}: Score file not found\")\n",
    "    \n",
    "    df = pd.DataFrame(score_data)\n",
    "    print(f\"\\nðŸ“ˆ Score DataFrame created with {len(df)} rows\")\n",
    "    print(f\"Models: {sorted(df['model'].unique())}\")\n",
    "    print(f\"Test categories: {sorted(df['test_category'].unique())}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the score data\n",
    "print(\"Loading model-specific score data...\")\n",
    "scores_df = build_score_dataframe(score_folder='bfcl/berkeley-function-call-leaderboard/score')\n",
    "print(\"\\nâœ… Score data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Score Summary Table\n",
    "\n",
    "Let's examine the accuracy scores for each model across all test categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the complete score data\n",
    "print(\"ðŸŽ¯ Model Performance Summary:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a pivot table for better readability\n",
    "pivot_df = scores_df.pivot(index='model', columns='test_category', values='accuracy_percentage')\n",
    "pivot_df = pivot_df.round(2)\n",
    "\n",
    "# Add overall average\n",
    "pivot_df['average'] = pivot_df.mean(axis=1).round(2)\n",
    "\n",
    "# Sort by average performance\n",
    "pivot_df = pivot_df.sort_values('average', ascending=False)\n",
    "\n",
    "print(pivot_df)\n",
    "print(\"\\nðŸ“Š Values shown as accuracy percentages (%)\")\n",
    "\n",
    "# Also show the raw DataFrame\n",
    "print(\"\\nðŸ“ˆ Detailed Score Data:\")\n",
    "scores_df_display = scores_df.copy()\n",
    "scores_df_display['accuracy'] = scores_df_display['accuracy'].round(4)\n",
    "scores_df_display['accuracy_percentage'] = scores_df_display['accuracy_percentage'].round(2)\n",
    "scores_df_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Performance Visualization\n",
    "\n",
    "### Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overall performance comparison chart\n",
    "fig = px.bar(\n",
    "    scores_df, \n",
    "    x='model', \n",
    "    y='accuracy_percentage',\n",
    "    color='test_category',\n",
    "    title='ðŸŽ¯ Model Performance Across Test Categories',\n",
    "    labels={\n",
    "        'accuracy_percentage': 'Accuracy (%)',\n",
    "        'model': 'Model',\n",
    "        'test_category': 'Test Category'\n",
    "    },\n",
    "    color_discrete_map={\n",
    "        'simple': '#2E86AB',\n",
    "        'multiple': '#A23B72', \n",
    "        'live_relevance': '#F18F01',\n",
    "        'irrelevance': '#C73E1D'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45,\n",
    "    showlegend=True,\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category-by-Category Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for each test category\n",
    "categories = sorted(scores_df['test_category'].unique())\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[f'{cat.replace(\"_\", \" \").title()} Test Category' for cat in categories],\n",
    "    vertical_spacing=0.15,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    cat_data = scores_df[scores_df['test_category'] == category].sort_values('accuracy_percentage', ascending=True)\n",
    "    \n",
    "    row = (i // 2) + 1\n",
    "    col = (i % 2) + 1\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=cat_data['accuracy_percentage'],\n",
    "            y=cat_data['model'],\n",
    "            orientation='h',\n",
    "            name=category,\n",
    "            marker_color=colors[i],\n",
    "            showlegend=False,\n",
    "            text=cat_data['accuracy_percentage'].round(1),\n",
    "            textposition='outside'\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Accuracy (%)\", row=row, col=col)\n",
    "    fig.update_yaxes(title_text=\"Model\", row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    title_text=\"ðŸ“Š Performance Analysis by Test Category\",\n",
    "    showlegend=False,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Ranking Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance and ranking\n",
    "avg_performance = scores_df.groupby('model')['accuracy_percentage'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"ðŸ† MODEL RANKINGS BY AVERAGE ACCURACY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for rank, (model, avg_acc) in enumerate(avg_performance.items(), 1):\n",
    "    print(f\"{rank}. {model:<20} {avg_acc:6.2f}%\")\n",
    "\n",
    "# Create ranking visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_performance.values,\n",
    "    y=avg_performance.index,\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=avg_performance.values,\n",
    "        colorscale='RdYlBu_r',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Accuracy (%)\")\n",
    "    ),\n",
    "    text=[f\"{val:.1f}%\" for val in avg_performance.values],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ðŸ† Overall Model Ranking by Average Accuracy',\n",
    "    xaxis_title='Average Accuracy (%)',\n",
    "    yaxis_title='Model',\n",
    "    height=400,\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Performance Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive statistics\n",
    "print(\"ðŸ“Š COMPREHENSIVE PERFORMANCE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall statistics\n",
    "overall_stats = scores_df['accuracy_percentage'].describe()\n",
    "print(\"\\nðŸŽ¯ Overall Accuracy Statistics:\")\n",
    "for stat, value in overall_stats.items():\n",
    "    print(f\"{stat.capitalize():<10}: {value:6.2f}%\")\n",
    "\n",
    "# Best and worst performers by category\n",
    "print(\"\\nðŸ† Best Performers by Category:\")\n",
    "for category in sorted(scores_df['test_category'].unique()):\n",
    "    cat_data = scores_df[scores_df['test_category'] == category]\n",
    "    best = cat_data.loc[cat_data['accuracy_percentage'].idxmax()]\n",
    "    print(f\"{category.replace('_', ' ').title():<15}: {best['model']:<20} ({best['accuracy_percentage']:.2f}%)\")\n",
    "\n",
    "print(\"\\nðŸ“‰ Performance Ranges by Category:\")\n",
    "for category in sorted(scores_df['test_category'].unique()):\n",
    "    cat_data = scores_df[scores_df['test_category'] == category]['accuracy_percentage']\n",
    "    print(f\"{category.replace('_', ' ').title():<15}: {cat_data.min():.2f}% - {cat_data.max():.2f}% (range: {cat_data.max()-cat_data.min():.2f}%)\")\n",
    "\n",
    "# Model consistency analysis\n",
    "print(\"\\nðŸ“Š Model Consistency (Standard Deviation):\")\n",
    "model_std = scores_df.groupby('model')['accuracy_percentage'].std().sort_values()\n",
    "for model, std in model_std.items():\n",
    "    consistency = \"High\" if std < 5 else \"Medium\" if std < 10 else \"Low\"\n",
    "    print(f\"{model:<20}: {std:5.2f}% ({consistency} consistency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Detailed Analysis Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights and observations\n",
    "print(\"ðŸ” KEY INSIGHTS FROM FUNCTION CALLING EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find top performer\n",
    "top_model = avg_performance.index[0]\n",
    "top_score = avg_performance.iloc[0]\n",
    "print(f\"\\nðŸ¥‡ Top Performer: {top_model} with {top_score:.2f}% average accuracy\")\n",
    "\n",
    "# Find most challenging category\n",
    "category_avg = scores_df.groupby('test_category')['accuracy_percentage'].mean().sort_values()\n",
    "hardest_category = category_avg.index[0]\n",
    "hardest_score = category_avg.iloc[0]\n",
    "easiest_category = category_avg.index[-1]\n",
    "easiest_score = category_avg.iloc[-1]\n",
    "\n",
    "print(f\"\\nðŸ“Š Most Challenging Category: {hardest_category.replace('_', ' ').title()} ({hardest_score:.2f}% avg)\")\n",
    "print(f\"ðŸ“Š Easiest Category: {easiest_category.replace('_', ' ').title()} ({easiest_score:.2f}% avg)\")\n",
    "\n",
    "# Custom model performance\n",
    "if 'custom-nova-lite' in scores_df['model'].values:\n",
    "    custom_avg = avg_performance['custom-nova-lite']\n",
    "    custom_rank = list(avg_performance.index).index('custom-nova-lite') + 1\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Custom Model Performance:\")\n",
    "    print(f\"   custom-nova-lite ranks #{custom_rank} with {custom_avg:.2f}% average accuracy\")\n",
    "    \n",
    "    # Compare with base models\n",
    "    if 'nova-lite-v1.0' in avg_performance.index:\n",
    "        base_lite_avg = avg_performance['nova-lite-v1.0']\n",
    "        improvement = custom_avg - base_lite_avg\n",
    "        direction = \"improvement\" if improvement > 0 else \"decrease\"\n",
    "        print(f\"   vs nova-lite-v1.0: {abs(improvement):.2f}% {direction}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Performance Distribution:\")\n",
    "high_performers = (avg_performance >= 90).sum()\n",
    "medium_performers = ((avg_performance >= 80) & (avg_performance < 90)).sum()\n",
    "low_performers = (avg_performance < 80).sum()\n",
    "\n",
    "print(f\"   High (â‰¥90%): {high_performers} models\")\n",
    "print(f\"   Medium (80-90%): {medium_performers} models\")\n",
    "print(f\"   Lower (<80%): {low_performers} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Export Results\n",
    "\n",
    "Save the analysis results for further use or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV\n",
    "output_dir = 'analysis_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export detailed scores\n",
    "scores_df.to_csv(f'{output_dir}/model_scores_detailed.csv', index=False)\n",
    "print(f\"âœ… Detailed scores exported to {output_dir}/model_scores_detailed.csv\")\n",
    "\n",
    "# Export pivot table\n",
    "pivot_df.to_csv(f'{output_dir}/model_scores_pivot.csv')\n",
    "print(f\"âœ… Pivot table exported to {output_dir}/model_scores_pivot.csv\")\n",
    "\n",
    "# Export rankings\n",
    "ranking_df = pd.DataFrame({\n",
    "    'model': avg_performance.index,\n",
    "    'average_accuracy': avg_performance.values,\n",
    "    'rank': range(1, len(avg_performance) + 1)\n",
    "})\n",
    "ranking_df.to_csv(f'{output_dir}/model_rankings.csv', index=False)\n",
    "print(f\"âœ… Rankings exported to {output_dir}/model_rankings.csv\")\n",
    "\n",
    "print(f\"\\nðŸ“ All analysis files saved to '{output_dir}/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "Now that you've completed the function calling evaluation analysis, here are recommended next steps:\n",
    "\n",
    "### 1. **Model Selection and Optimization**\n",
    "- Review the model rankings and select the best-performing model for your use case\n",
    "- Consider the trade-offs between accuracy and model size/cost\n",
    "- If using custom-nova-lite, analyze its performance relative to base models\n",
    "\n",
    "### 2. **Deep Dive into Specific Categories**\n",
    "- Investigate why certain test categories are more challenging\n",
    "- Examine individual test cases in categories where models struggle\n",
    "- Consider additional training or fine-tuning for underperforming categories\n",
    "\n",
    "### 3. **Production Deployment Planning**\n",
    "- Use the consistency analysis to understand model reliability\n",
    "- Plan fallback strategies for function calling failures\n",
    "- Consider ensemble approaches using multiple models\n",
    "\n",
    "### 4. **Further Analysis Options**\n",
    "- Analyze error patterns from the detailed score files\n",
    "- Compare token usage and latency across models (from result files)\n",
    "- Conduct cost-benefit analysis including inference costs\n",
    "\n",
    "### 5. **Documentation and Reporting**\n",
    "- Use the exported CSV files for stakeholder reports\n",
    "- Document model selection rationale\n",
    "- Create monitoring dashboards for production function calling performance\n",
    "\n",
    "### ðŸ“š **Additional Resources**\n",
    "- [Berkeley Function Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) - Official BFCL leaderboard\n",
    "- [Amazon Bedrock Custom Models](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html) - Documentation for custom model development\n",
    "- [Function Calling Best Practices](https://docs.aws.amazon.com/bedrock/latest/userguide/function-calling.html) - AWS Bedrock function calling guidance\n",
    "\n",
    "### ðŸ› ï¸ **Tools for Continued Analysis**\n",
    "- Review notebook `01_prepare_data.ipynb` for data preparation insights\n",
    "- Use notebook `02_generate_responses.ipynb` for generating additional test cases\n",
    "- Examine notebook `03_evaluate_responses.ipynb` for evaluation methodology\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis focused on model-specific performance using official BFCL evaluation scores for the four test categories: irrelevance, live_relevance, multiple, and simple.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02_function_calling-tDu3jLPM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
