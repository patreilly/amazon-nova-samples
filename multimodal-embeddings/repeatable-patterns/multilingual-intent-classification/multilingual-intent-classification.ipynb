{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Intent Classification with Amazon Nova Multimodal Embeddings\n",
    "\n",
    "Customer service systems receive requests in multiple languages daily. A user might say \"I want to cancel my order\" in English, \"Quiero cancelar mi pedido\" in Spanish, or \"Je veux annuler ma commande\" in French. Each expresses the same intent but uses completely different words and grammar structures.\n",
    "\n",
    "Traditional intent classification systems struggle with this linguistic diversity. Most approaches require training separate models for each language or translating everything to English first. Both solutions introduce complexity, cost, and potential errors.\n",
    "\n",
    "This notebook explores a different approach using Amazon Nova Multimodal Embeddings. Instead of focusing on words, we'll work with semantic embeddings‚Äîmathematical representations that capture meaning across languages. When Nova processes \"cancel order\" in English and \"cancelar pedido\" in Spanish, it creates similar vector representations because the underlying intent is identical.\n",
    "\n",
    "The system we'll build uses these embeddings with a K-Nearest Neighbors classifier. The classifier finds training examples with similar meanings to new inputs, regardless of language. This approach allows us to train once and classify everywhere, eliminating the need for language-specific models or translation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by installing the required Python libraries and importing the modules we'll need throughout the implementation. The installation includes boto3 for AWS services, scikit-learn for machine learning operations, and visualization libraries for analyzing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 pandas numpy scikit-learn matplotlib seaborn tqdm langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure our AWS environment and set up the connection to Nova Multimodal Embeddings. The configuration specifies us-east-1 as our region since Nova is currently available there. We also set the embedding dimension to 1024, which provides the best balance between representational capacity and computational efficiency. The S3 bucket name includes a hash to ensure uniqueness across different users running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_REGION = 'us-east-1'\n",
    "S3_BUCKET_NAME = 'nova-multilingual-classification-' + str(hash(os.getlogin()))[-6:]\n",
    "DATASET_PATH = './massive'\n",
    "\n",
    "try:\n",
    "    bedrock_client = boto3.client('bedrock-runtime', region_name=AWS_REGION)\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "    s3vectors_client = boto3.client('s3vectors', region_name=AWS_REGION)\n",
    "    print('AWS clients initialized')\n",
    "except Exception as e:\n",
    "    print(f'AWS client initialization failed: {e}')\n",
    "    print('Ensure AWS credentials are configured')\n",
    "\n",
    "NOVA_MODEL_ID = 'amazon.nova-2-multimodal-embeddings-v1:0'\n",
    "EMBEDDING_DIMENSION = 1024\n",
    "\n",
    "print(f'Region: {AWS_REGION}')\n",
    "print(f'Model: {NOVA_MODEL_ID}')\n",
    "print(f'Embedding dimension: {EMBEDDING_DIMENSION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose our dataset strategy. The MASSIVE dataset contains over one million samples across 52 languages, making it ideal for production systems but expensive for learning purposes. Our sample dataset contains just 12 carefully chosen examples across 4 languages, which demonstrates the core concepts while minimizing API costs. For this tutorial, we'll use the sample data to keep costs low while still showing how the system works across multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MASSIVE_DATASET = False\n",
    "\n",
    "MASSIVE_URL = 'https://amazon-massive-nlu-dataset.s3.amazonaws.com/amazon-massive-dataset-1.0.tar.gz'\n",
    "MASSIVE_ARCHIVE = 'amazon-massive-dataset-1.0.tar.gz'\n",
    "MASSIVE_DATA_DIR = '1.0/data'\n",
    "SAMPLE_LANGUAGES = ['en-US', 'es-ES', 'fr-FR', 'de-DE']\n",
    "\n",
    "dataset_mode = \"MASSIVE\" if USE_MASSIVE_DATASET else \"Sample\"\n",
    "print(f'Dataset mode: {dataset_mode}')\n",
    "if USE_MASSIVE_DATASET:\n",
    "    print(f'Languages: {len(SAMPLE_LANGUAGES)} of 52 available')\n",
    "else:\n",
    "    print(f'Languages: 4 sample languages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the core system architecture. The NovaEmbeddings class handles all communication with the Bedrock API, managing request formatting, error handling, and cost tracking. Each text input gets converted into a 1024-dimensional vector that captures its semantic meaning. The class includes batch processing capabilities with progress tracking, which becomes important when processing larger datasets. Request counting helps monitor API usage for cost management in production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovaEmbeddings:\n",
    "    def __init__(self, region='us-east-1'):\n",
    "        self.bedrock = boto3.client('bedrock-runtime', region_name=region)\n",
    "        self.model_id = 'amazon.nova-2-multimodal-embeddings-v1:0'\n",
    "        self.request_count = 0\n",
    "    \n",
    "    def embed_text(self, text: str, dimension: int = 1024) -> List[float]:\n",
    "        if not text or not text.strip():\n",
    "            return [0.0] * dimension\n",
    "            \n",
    "        request_body = {\n",
    "            'taskType': 'SINGLE_EMBEDDING',\n",
    "            'singleEmbeddingParams': {\n",
    "                'embeddingDimension': dimension,\n",
    "                'embeddingPurpose': 'CLASSIFICATION',\n",
    "                'text': {\"truncationMode\": \"END\", 'value': text[:8000]}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.bedrock.invoke_model(\n",
    "                modelId=self.model_id,\n",
    "                body=json.dumps(request_body)\n",
    "            )\n",
    "            result = json.loads(response['body'].read())\n",
    "            self.request_count += 1\n",
    "            return result['embeddings'][0]['embedding']\n",
    "        except Exception as e:\n",
    "            print(f'Embedding error: {e}')\n",
    "            return [0.0] * dimension\n",
    "    \n",
    "    def embed_batch_text(self, texts: List[str], dimension: int = 1024) -> List[List[float]]:\n",
    "        embeddings = []\n",
    "        for text in tqdm(texts, desc='Generating embeddings'):\n",
    "            embedding = self.embed_text(text, dimension)\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MultilingualClassifier class implements our intent recognition logic using K-Nearest Neighbors with cosine similarity. KNN works particularly well for this task because similar intents cluster together in the embedding space, regardless of the language used to express them. The classifier stores embeddings from training examples and finds the k most similar examples when making predictions. Cosine similarity measures the angle between vectors, which captures semantic similarity better than Euclidean distance for high-dimensional embeddings. The class provides both single predictions with confidence scores and batch processing for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualClassifier:\n",
    "    def __init__(self, embeddings_client, k_neighbors: int = 3):\n",
    "        self.embeddings = embeddings_client\n",
    "        self.classifier = KNeighborsClassifier(n_neighbors=k_neighbors, metric='cosine')\n",
    "        self.is_trained = False\n",
    "        self.intent_labels = []\n",
    "    \n",
    "    def train(self, texts: List[str], intents: List[str]):\n",
    "        print(f'Training classifier with {len(texts)} samples')\n",
    "        \n",
    "        embeddings = self.embeddings.embed_batch_text(texts)\n",
    "        self.classifier.fit(embeddings, intents)\n",
    "        self.intent_labels = list(set(intents))\n",
    "        self.is_trained = True\n",
    "        \n",
    "        print(f'Training complete: {len(self.intent_labels)} intent classes')\n",
    "    \n",
    "    def predict(self, text: str) -> Dict:\n",
    "        if not self.is_trained:\n",
    "            raise ValueError('Classifier must be trained first')\n",
    "        \n",
    "        embedding = self.embeddings.embed_text(text)\n",
    "        prediction = self.classifier.predict([embedding])[0]\n",
    "        probabilities = self.classifier.predict_proba([embedding])[0]\n",
    "        \n",
    "        class_scores = dict(zip(self.classifier.classes_, probabilities))\n",
    "        \n",
    "        return {\n",
    "            'predicted_intent': prediction,\n",
    "            'confidence': max(probabilities),\n",
    "            'all_scores': class_scores\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, texts: List[str]) -> List[str]:\n",
    "        if not self.is_trained:\n",
    "            raise ValueError('Classifier must be trained first')\n",
    "        \n",
    "        embeddings = self.embeddings.embed_batch_text(texts)\n",
    "        predictions = self.classifier.predict(embeddings)\n",
    "        return predictions.tolist()\n",
    "    \n",
    "    def evaluate(self, test_texts: List[str], true_intents: List[str]) -> Dict:\n",
    "        predictions = self.predict_batch(test_texts)\n",
    "        accuracy = accuracy_score(true_intents, predictions)\n",
    "        report = classification_report(true_intents, predictions, output_dict=True)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report,\n",
    "            'predictions': predictions\n",
    "        }\n",
    "\n",
    "print('Classes defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data loading functions handle both the full MASSIVE dataset and our sample data. The MASSIVE dataset requires downloading and extracting a compressed archive, then parsing JSONL files for each language. Our sample data creation function builds a small but representative dataset with three common intents: setting alarms, querying weather, and playing music. Each intent appears in all four languages with natural variations in phrasing. This gives us enough data to demonstrate multilingual understanding while keeping API costs minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def download_massive_dataset():\n",
    "    if not os.path.exists(MASSIVE_ARCHIVE):\n",
    "        print(f'Downloading MASSIVE dataset from {MASSIVE_URL}')\n",
    "        urllib.request.urlretrieve(MASSIVE_URL, MASSIVE_ARCHIVE)\n",
    "        print('Download complete')\n",
    "    \n",
    "    if not os.path.exists(MASSIVE_DATA_DIR):\n",
    "        print('Extracting dataset')\n",
    "        with tarfile.open(MASSIVE_ARCHIVE, 'r:gz') as tar:\n",
    "            tar.extractall()\n",
    "        print('Extraction complete')\n",
    "\n",
    "def load_massive_data(languages=None, max_samples_per_lang=1000):\n",
    "    if languages is None:\n",
    "        languages = SAMPLE_LANGUAGES\n",
    "    \n",
    "    multilingual_data = {}\n",
    "    \n",
    "    for lang in languages:\n",
    "        jsonl_path = os.path.join(MASSIVE_DATA_DIR, f'{lang}.jsonl')\n",
    "        if os.path.exists(jsonl_path):\n",
    "            data = []\n",
    "            with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if i >= max_samples_per_lang:\n",
    "                        break\n",
    "                    data.append(json.loads(line.strip()))\n",
    "            multilingual_data[lang] = pd.DataFrame(data)\n",
    "            print(f'{lang}: {len(data)} samples loaded')\n",
    "        else:\n",
    "            print(f'{lang}: File not found')\n",
    "    \n",
    "    return multilingual_data\n",
    "\n",
    "def create_sample_data():\n",
    "    sample_data = {\n",
    "        'en-US': [\n",
    "            {'id': '1', 'locale': 'en-US', 'partition': 'train', 'intent': 'alarm_set', 'scenario': 'alarm', 'utt': 'set alarm for 7 AM tomorrow'},\n",
    "            {'id': '2', 'locale': 'en-US', 'partition': 'train', 'intent': 'weather_query', 'scenario': 'weather', 'utt': 'what is the weather like today'},\n",
    "            {'id': '3', 'locale': 'en-US', 'partition': 'train', 'intent': 'music_play', 'scenario': 'music', 'utt': 'play some jazz music'},\n",
    "        ],\n",
    "        'es-ES': [\n",
    "            {'id': '1', 'locale': 'es-ES', 'partition': 'train', 'intent': 'alarm_set', 'scenario': 'alarm', 'utt': 'pon la alarma para las 7 de la ma√±ana'},\n",
    "            {'id': '2', 'locale': 'es-ES', 'partition': 'train', 'intent': 'weather_query', 'scenario': 'weather', 'utt': '¬øc√≥mo est√° el tiempo hoy?'},\n",
    "            {'id': '3', 'locale': 'es-ES', 'partition': 'train', 'intent': 'music_play', 'scenario': 'music', 'utt': 'reproduce m√∫sica jazz'},\n",
    "        ],\n",
    "        'fr-FR': [\n",
    "            {'id': '1', 'locale': 'fr-FR', 'partition': 'train', 'intent': 'alarm_set', 'scenario': 'alarm', 'utt': 'r√®gle l\\'alarme pour 7 heures demain matin'},\n",
    "            {'id': '2', 'locale': 'fr-FR', 'partition': 'train', 'intent': 'weather_query', 'scenario': 'weather', 'utt': 'quel temps fait-il aujourd\\'hui'},\n",
    "            {'id': '3', 'locale': 'fr-FR', 'partition': 'train', 'intent': 'music_play', 'scenario': 'music', 'utt': 'joue de la musique jazz'},\n",
    "        ],\n",
    "        'de-DE': [\n",
    "            {'id': '1', 'locale': 'de-DE', 'partition': 'train', 'intent': 'alarm_set', 'scenario': 'alarm', 'utt': 'stelle den Wecker auf 7 Uhr morgen fr√ºh'},\n",
    "            {'id': '2', 'locale': 'de-DE', 'partition': 'train', 'intent': 'weather_query', 'scenario': 'weather', 'utt': 'wie ist das Wetter heute'},\n",
    "            {'id': '3', 'locale': 'de-DE', 'partition': 'train', 'intent': 'music_play', 'scenario': 'music', 'utt': 'spiele Jazz-Musik'},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    multilingual_data = {}\n",
    "    for lang, samples in sample_data.items():\n",
    "        multilingual_data[lang] = pd.DataFrame(samples)\n",
    "    \n",
    "    return multilingual_data\n",
    "\n",
    "if USE_MASSIVE_DATASET:\n",
    "    print('Loading MASSIVE dataset')\n",
    "    download_massive_dataset()\n",
    "    multilingual_data = load_massive_data(SAMPLE_LANGUAGES, max_samples_per_lang=100)\n",
    "    print(f'Loaded MASSIVE data for {len(multilingual_data)} languages')\n",
    "else:\n",
    "    print('Creating sample data')\n",
    "    multilingual_data = create_sample_data()\n",
    "    print(f'Created sample data for {len(multilingual_data)} languages')\n",
    "\n",
    "total_samples = sum(len(df) for df in multilingual_data.values())\n",
    "print(f'\\nDataset summary:')\n",
    "for lang, df in multilingual_data.items():\n",
    "    print(f'{lang}: {len(df)} samples')\n",
    "print(f'Total: {total_samples} samples')\n",
    "\n",
    "all_intents = []\n",
    "for df in multilingual_data.values():\n",
    "    all_intents.extend(df['intent'].tolist())\n",
    "intent_counts = pd.Series(all_intents).value_counts()\n",
    "print(f'\\nIntent distribution:')\n",
    "for intent, count in intent_counts.items():\n",
    "    print(f'{intent}: {count} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data loaded, we can now initialize and train our multilingual classification system. The training process involves two main steps: first, we create instances of our NovaEmbeddings and MultilingualClassifier classes, then we combine all text samples from all languages into a single training set. This approach allows the classifier to learn from examples across languages simultaneously, which improves its ability to recognize similar intents regardless of the language used to express them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initializing system components')\n",
    "\n",
    "nova_embeddings = NovaEmbeddings(region=AWS_REGION)\n",
    "classifier = MultilingualClassifier(nova_embeddings, k_neighbors=3)\n",
    "\n",
    "print('System initialized')\n",
    "\n",
    "train_texts = []\n",
    "train_intents = []\n",
    "\n",
    "for language, df in multilingual_data.items():\n",
    "    for _, row in df.iterrows():\n",
    "        train_texts.append(row['utt'])\n",
    "        train_intents.append(row['intent'])\n",
    "\n",
    "print(f'Prepared {len(train_texts)} training samples across {len(multilingual_data)} languages')\n",
    "\n",
    "classifier.train(train_texts, train_intents)\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test our trained classifier with queries in different languages to verify that it can correctly identify intents across linguistic boundaries. The test queries include both complete phrases and partial expressions to see how well the system handles different levels of specificity. Each prediction returns not just the most likely intent, but also confidence scores for all possible intents, which helps us understand how certain the model is about each classification decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    ('set an alarm for 8 AM', 'en-US'),\n",
    "    ('¬øqu√© tiempo hace?', 'es-ES'),\n",
    "    ('joue ma chanson pr√©f√©r√©e', 'fr-FR'),\n",
    "    ('Wecker f√ºr morgen fr√ºh', 'de-DE'),\n",
    "    ('play rock music', 'en-US'),\n",
    "    ('tiempo ma√±ana', 'es-ES')\n",
    "]\n",
    "\n",
    "print('multilingual classification test\\n')\n",
    "\n",
    "for i, (query, lang) in enumerate(test_queries, 1):\n",
    "    print(f'Query {i}: \"{query}\" ({lang})')\n",
    "    \n",
    "    result = classifier.predict(query)\n",
    "    \n",
    "    print(f'Predicted intent: {result[\"predicted_intent\"]}')\n",
    "    print(f'Confidence: {result[\"confidence\"]:.4f}')\n",
    "    \n",
    "    print('All scores:')\n",
    "    for intent, score in sorted(result['all_scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f'  {intent}: {score:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly evaluate multilingual transfer capabilities, we need to test whether knowledge learned from one language can be applied to others. This experiment trains a new classifier using only English examples, then tests it on Spanish, French, and German inputs. This simulates a real-world scenario where you might have abundant labeled data in one language but need to support users speaking other languages. Strong performance in this test indicates that the embeddings truly capture semantic meaning rather than language-specific patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('multilingual transfer evaluation')\n",
    "print('Training on English, testing on other languages\\n')\n",
    "\n",
    "en_df = multilingual_data['en-US']\n",
    "en_texts = en_df['utt'].tolist()\n",
    "en_intents = en_df['intent'].tolist()\n",
    "\n",
    "en_classifier = MultilingualClassifier(nova_embeddings, k_neighbors=1)\n",
    "en_classifier.train(en_texts, en_intents)\n",
    "\n",
    "transfer_results = {}\n",
    "for test_lang in ['es-ES', 'fr-FR', 'de-DE']:\n",
    "    if test_lang in multilingual_data:\n",
    "        test_df = multilingual_data[test_lang]\n",
    "        test_texts = test_df['utt'].tolist()\n",
    "        test_intents = test_df['intent'].tolist()\n",
    "        \n",
    "        print(f'Testing transfer to {test_lang}:')\n",
    "        \n",
    "        results = en_classifier.evaluate(test_texts, test_intents)\n",
    "        transfer_results[test_lang] = results\n",
    "        \n",
    "        print(f'Accuracy: {results[\"accuracy\"]:.4f} ({results[\"accuracy\"]*100:.1f}%)')\n",
    "        print(f'Macro F1: {results[\"classification_report\"][\"macro avg\"][\"f1-score\"]:.4f}')\n",
    "        \n",
    "        print('Per-intent performance:')\n",
    "        for intent in ['alarm_set', 'weather_query', 'music_play']:\n",
    "            if intent in results['classification_report']:\n",
    "                f1 = results['classification_report'][intent]['f1-score']\n",
    "                print(f'  {intent}: F1={f1:.4f}')\n",
    "        print()\n",
    "\n",
    "avg_accuracy = np.mean([r['accuracy'] for r in transfer_results.values()])\n",
    "print(f'Average multilingual accuracy: {avg_accuracy:.4f} ({avg_accuracy*100:.1f}%)')\n",
    "print('multilingual evaluation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how Nova represents the same intent across different languages provides insight into why multilingual classification works. We'll measure the cosine similarity between embeddings of the same intent expressed in different languages. High similarity scores indicate that Nova creates similar vector representations for semantically equivalent phrases, regardless of the specific words or grammar used. This analysis helps validate that the embeddings capture meaning rather than surface linguistic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('multilingual semantic similarity analysis\\n')\n",
    "\n",
    "intent_groups = {}\n",
    "for lang, df in multilingual_data.items():\n",
    "    for _, row in df.iterrows():\n",
    "        intent = row['intent']\n",
    "        if intent not in intent_groups:\n",
    "            intent_groups[intent] = {}\n",
    "        intent_groups[intent][lang] = row['utt']\n",
    "\n",
    "intent_similarities = {}\n",
    "for intent, lang_texts in intent_groups.items():\n",
    "    print(f'Intent: {intent}')\n",
    "    print(f'Languages: {\", \".join(lang_texts.keys())}')\n",
    "    \n",
    "    if len(lang_texts) >= 2:\n",
    "        texts = list(lang_texts.values())\n",
    "        langs = list(lang_texts.keys())\n",
    "        \n",
    "        embeddings = nova_embeddings.embed_batch_text(texts)\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "        \n",
    "        print('multilingual similarities:')\n",
    "        intent_sims = []\n",
    "        for i in range(len(langs)):\n",
    "            for j in range(i+1, len(langs)):\n",
    "                sim = similarities[i][j]\n",
    "                intent_sims.append(sim)\n",
    "                print(f'  {langs[i]} - {langs[j]}: {sim:.4f}')\n",
    "        \n",
    "        avg_similarity = np.mean(intent_sims)\n",
    "        intent_similarities[intent] = avg_similarity\n",
    "        print(f'Average similarity: {avg_similarity:.4f}')\n",
    "    \n",
    "    print()\n",
    "\n",
    "if intent_similarities:\n",
    "    overall_similarity = np.mean(list(intent_similarities.values()))\n",
    "    print(f'Overall multilingual semantic consistency: {overall_similarity:.4f}')\n",
    "\n",
    "print('Similarity analysis complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a complete performance assessment, we need to evaluate the system using standard machine learning metrics and analyze the cost implications of our approach. When we have sufficient data, we'll split it into training and testing sets to get unbiased performance estimates. The evaluation includes accuracy, precision, recall, and F1 scores, which provide different perspectives on classification performance. We also track API usage to understand the cost structure for production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance analysis\\n')\n",
    "\n",
    "all_texts = []\n",
    "all_intents = []\n",
    "\n",
    "for language, df in multilingual_data.items():\n",
    "    for _, row in df.iterrows():\n",
    "        all_texts.append(row['utt'])\n",
    "        all_intents.append(row['intent'])\n",
    "\n",
    "if len(all_texts) > 6:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        all_texts, all_intents, test_size=0.3, random_state=42, stratify=all_intents\n",
    "    )\n",
    "    \n",
    "    eval_classifier = MultilingualClassifier(nova_embeddings, k_neighbors=3)\n",
    "    eval_classifier.train(X_train, y_train)\n",
    "    \n",
    "    results = eval_classifier.evaluate(X_test, y_test)\n",
    "    \n",
    "    print('Classification performance:')\n",
    "    print(f'Overall accuracy: {results[\"accuracy\"]:.4f} ({results[\"accuracy\"]*100:.2f}%)')\n",
    "    print(f'Macro F1: {results[\"classification_report\"][\"macro avg\"][\"f1-score\"]:.4f}')\n",
    "    print(f'Weighted F1: {results[\"classification_report\"][\"weighted avg\"][\"f1-score\"]:.4f}')\n",
    "        \n",
    "else:\n",
    "    print('Small dataset - using full dataset evaluation')\n",
    "    results = classifier.evaluate(all_texts, all_intents)\n",
    "    print(f'Full dataset accuracy: {results[\"accuracy\"]:.4f} ({results[\"accuracy\"]*100:.2f}%)')\n",
    "\n",
    "print(f'\\nCost analysis:')\n",
    "print(f'Total Nova API calls: {nova_embeddings.request_count}')\n",
    "print(f'Estimated cost: ${nova_embeddings.request_count * 0.0002:.4f}')\n",
    "print(f'Cost per sample: ${(nova_embeddings.request_count * 0.0002) / len(all_texts):.6f}')\n",
    "\n",
    "print(f'\\nSystem specifications:')\n",
    "print(f'Languages supported: {len(multilingual_data)}')\n",
    "print(f'Intent classes: {len(set(all_intents))}')\n",
    "print(f'Training samples: {len(all_texts)}')\n",
    "\n",
    "print('Performance analysis complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our evaluation meets industry standards, we'll implement a comprehensive assessment following MTEB (Massive Text Embedding Benchmark) methodology. This includes stratified sampling to maintain class balance, multiple evaluation metrics, and systematic multilingual transfer analysis. The evaluation functions we'll define handle both single-language performance assessment and multilingual transfer matrix generation, which shows how well each language transfers to every other language in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def comprehensive_evaluation(texts, labels, test_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=test_size, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    eval_classifier = MultilingualClassifier(nova_embeddings, k_neighbors=3)\n",
    "    eval_classifier.train(X_train, y_train)\n",
    "    \n",
    "    y_pred = eval_classifier.predict_batch(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_names': list(set(labels))\n",
    "    }\n",
    "\n",
    "def cross_lingual_transfer_matrix(multilingual_data, embeddings_client):\n",
    "    languages = list(multilingual_data.keys())\n",
    "    results_matrix = np.zeros((len(languages), len(languages)))\n",
    "    \n",
    "    for i, train_lang in enumerate(languages):\n",
    "        train_df = multilingual_data[train_lang]\n",
    "        train_texts = train_df['utt'].tolist()\n",
    "        train_labels = train_df['intent'].tolist()\n",
    "        \n",
    "        classifier = MultilingualClassifier(embeddings_client, k_neighbors=1)\n",
    "        classifier.train(train_texts, train_labels)\n",
    "        \n",
    "        for j, test_lang in enumerate(languages):\n",
    "            test_df = multilingual_data[test_lang]\n",
    "            test_texts = test_df['utt'].tolist()\n",
    "            test_labels = test_df['intent'].tolist()\n",
    "            \n",
    "            predictions = classifier.predict_batch(test_texts)\n",
    "            accuracy = accuracy_score(test_labels, predictions)\n",
    "            results_matrix[i, j] = accuracy\n",
    "    \n",
    "    return results_matrix, languages\n",
    "\n",
    "print('Running comprehensive evaluation\\n')\n",
    "\n",
    "all_texts = []\n",
    "all_labels = []\n",
    "\n",
    "for lang, df in multilingual_data.items():\n",
    "    for _, row in df.iterrows():\n",
    "        all_texts.append(row['utt'])\n",
    "        all_labels.append(row['intent'])\n",
    "\n",
    "if len(set(all_labels)) > 1 and len(all_texts) > 6:\n",
    "    print('Overall classification performance:')\n",
    "    \n",
    "    results = comprehensive_evaluation(all_texts, all_labels)\n",
    "    \n",
    "    print(f'Accuracy: {results[\"accuracy\"]:.4f} ({results[\"accuracy\"]*100:.2f}%)')\n",
    "    print(f'Macro F1: {results[\"macro_f1\"]:.4f}')\n",
    "    print(f'Micro F1: {results[\"micro_f1\"]:.4f}')\n",
    "    print(f'Weighted F1: {results[\"weighted_f1\"]:.4f}')\n",
    "    \n",
    "    print('\\nPer-class performance:')\n",
    "    for intent in results['class_names']:\n",
    "        if intent in results['classification_report']:\n",
    "            metrics = results['classification_report'][intent]\n",
    "            print(f'{intent:15} | F1: {metrics[\"f1-score\"]:.4f} | Precision: {metrics[\"precision\"]:.4f} | Recall: {metrics[\"recall\"]:.4f}')\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(results['confusion_matrix'], \n",
    "                annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=results['class_names'],\n",
    "                yticklabels=results['class_names'])\n",
    "    plt.title('Classification Confusion Matrix')\n",
    "    plt.ylabel('True Intent')\n",
    "    plt.xlabel('Predicted Intent')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('\\nmultilingual transfer analysis:')\n",
    "\n",
    "transfer_matrix, langs = cross_lingual_transfer_matrix(multilingual_data, nova_embeddings)\n",
    "\n",
    "print('Transfer accuracy matrix (train ‚Üí test):')\n",
    "print(f'{'':12}', end='')\n",
    "for lang in langs:\n",
    "    print(f'{lang:8}', end='')\n",
    "print()\n",
    "\n",
    "for i, train_lang in enumerate(langs):\n",
    "    print(f'{train_lang:12}', end='')\n",
    "    for j, test_lang in enumerate(langs):\n",
    "        print(f'{transfer_matrix[i,j]:8.3f}', end='')\n",
    "    print()\n",
    "\n",
    "same_lang_acc = np.mean([transfer_matrix[i,i] for i in range(len(langs))])\n",
    "cross_lang_acc = np.mean([transfer_matrix[i,j] for i in range(len(langs)) for j in range(len(langs)) if i != j])\n",
    "\n",
    "print(f'\\nSame-language accuracy: {same_lang_acc:.4f} ({same_lang_acc*100:.1f}%)')\n",
    "print(f'Multilingual accuracy: {cross_lang_acc:.4f} ({cross_lang_acc*100:.1f}%)')\n",
    "print(f'Transfer efficiency: {cross_lang_acc/same_lang_acc:.4f} ({cross_lang_acc/same_lang_acc*100:.1f}%)')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(transfer_matrix, \n",
    "            annot=True, fmt='.3f', cmap='RdYlBu_r',\n",
    "            xticklabels=langs, yticklabels=langs,\n",
    "            vmin=0, vmax=1)\n",
    "plt.title('multilingual Transfer Matrix')\n",
    "plt.ylabel('Training Language')\n",
    "plt.xlabel('Test Language')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Comprehensive evaluation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Multilingual Utterance Grouping Analysis\n",
    "\n",
    "Analyze utterance patterns across 10 languages to find the most common intents and expressions:\n",
    "- **Load 10 Languages**: Expand to top 10 languages from MASSIVE dataset\n",
    "- **Utterance Grouping**: Group equivalent utterances across languages by intent\n",
    "- **Frequency Ranking**: Rank utterance groups by frequency across all languages\n",
    "- **multilingual Patterns**: Identify most common expressions globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 10 languages for comprehensive analysis\n",
    "TOP_10_LANGUAGES = ['en-US', 'es-ES', 'fr-FR', 'de-DE', 'it-IT', 'pt-PT', 'nl-NL', 'pl-PL', 'ru-RU', 'ja-JP']\n",
    "\n",
    "def load_expanded_multilingual_data(languages, max_samples_per_lang=200):\n",
    "    \"\"\"Load data for 10 languages\"\"\"\n",
    "    if USE_MASSIVE_DATASET:\n",
    "        print('üìä Loading MASSIVE dataset for 10 languages...')\n",
    "        download_massive_dataset()\n",
    "        return load_massive_data(languages, max_samples_per_lang)\n",
    "    else:\n",
    "        # Create expanded sample data for 10 languages\n",
    "        expanded_data = {}\n",
    "        base_intents = {\n",
    "            'alarm_set': ['set alarm for 7 AM', 'wake me up at 7', 'alarm for tomorrow morning'],\n",
    "            'weather_query': ['what is the weather', 'how is the weather today', 'weather forecast'],\n",
    "            'music_play': ['play music', 'start some music', 'play my playlist'],\n",
    "            'calendar_query': ['what is my schedule', 'show my calendar', 'any meetings today'],\n",
    "            'timer_set': ['set timer for 5 minutes', 'start a timer', 'timer for cooking']\n",
    "        }\n",
    "        \n",
    "        for lang in languages[:4]:  # Use existing 4 languages\n",
    "            expanded_data[lang] = multilingual_data.get(lang, pd.DataFrame())\n",
    "        \n",
    "        # Add mock data for remaining languages\n",
    "        for i, lang in enumerate(languages[4:], 1):\n",
    "            samples = []\n",
    "            for j, (intent, utterances) in enumerate(base_intents.items(), 1):\n",
    "                samples.append({\n",
    "                    'id': str(j), 'locale': lang, 'partition': 'train',\n",
    "                    'intent': intent, 'scenario': intent.split('_')[0],\n",
    "                    'utt': f'{utterances[0]} ({lang})'\n",
    "                })\n",
    "            expanded_data[lang] = pd.DataFrame(samples)\n",
    "        \n",
    "        return expanded_data\n",
    "\n",
    "# Load expanded dataset\n",
    "expanded_multilingual_data = load_expanded_multilingual_data(TOP_10_LANGUAGES)\n",
    "\n",
    "print(f'\\nüìà Expanded Dataset Summary:')\n",
    "total_samples = 0\n",
    "for lang, df in expanded_multilingual_data.items():\n",
    "    print(f'  {lang}: {len(df)} samples')\n",
    "    total_samples += len(df)\n",
    "print(f'  Total: {total_samples} samples across {len(expanded_multilingual_data)} languages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze utterance groupings by intent across all languages\n",
    "print('üîç Utterance Grouping Analysis Across 10 Languages\\n')\n",
    "print('=' * 70)\n",
    "\n",
    "# Group utterances by intent across all languages\n",
    "intent_groups = {}\n",
    "intent_frequencies = {}\n",
    "\n",
    "for lang, df in expanded_multilingual_data.items():\n",
    "    for _, row in df.iterrows():\n",
    "        intent = row['intent']\n",
    "        utterance = row['utt']\n",
    "        \n",
    "        # Group by intent\n",
    "        if intent not in intent_groups:\n",
    "            intent_groups[intent] = {}\n",
    "            intent_frequencies[intent] = 0\n",
    "        \n",
    "        if lang not in intent_groups[intent]:\n",
    "            intent_groups[intent][lang] = []\n",
    "        \n",
    "        intent_groups[intent][lang].append(utterance)\n",
    "        intent_frequencies[intent] += 1\n",
    "\n",
    "# Rank intents by frequency\n",
    "ranked_intents = sorted(intent_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('üèÜ Intent Ranking by Frequency Across All Languages:')\n",
    "print(f'{'Rank':<4} {'Intent':<20} {'Total Count':<12} {'Languages':<10}')\n",
    "print('-' * 50)\n",
    "\n",
    "for rank, (intent, count) in enumerate(ranked_intents, 1):\n",
    "    lang_count = len(intent_groups[intent])\n",
    "    print(f'{rank:<4} {intent:<20} {count:<12} {lang_count:<10}')\n",
    "\n",
    "print(f'\\nüìä Detailed Utterance Groups (Top 3 Most Common Intents):')\n",
    "print('=' * 70)\n",
    "\n",
    "# Show detailed breakdown for top 3 intents\n",
    "for rank, (intent, count) in enumerate(ranked_intents[:3], 1):\n",
    "    print(f'\\nüéØ #{rank} Intent: {intent} (Total: {count} utterances across {len(intent_groups[intent])} languages)')\n",
    "    print('-' * 60)\n",
    "    \n",
    "    for lang in TOP_10_LANGUAGES:\n",
    "        if lang in intent_groups[intent]:\n",
    "            utterances = intent_groups[intent][lang]\n",
    "            print(f'  {lang}: {len(utterances)} utterances')\n",
    "            for utt in utterances[:2]:  # Show first 2 utterances per language\n",
    "                print(f'    ‚Ä¢ \"{utt}\"')\n",
    "            if len(utterances) > 2:\n",
    "                print(f'    ... and {len(utterances)-2} more')\n",
    "        else:\n",
    "            print(f'  {lang}: No data')\n",
    "\n",
    "print('\\n‚úÖ Utterance grouping analysis complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intent distribution across languages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create intent-language matrix for visualization\n",
    "intent_lang_matrix = []\n",
    "intent_names = []\n",
    "lang_names = TOP_10_LANGUAGES\n",
    "\n",
    "for intent, lang_data in intent_groups.items():\n",
    "    intent_names.append(intent)\n",
    "    row = []\n",
    "    for lang in lang_names:\n",
    "        count = len(lang_data.get(lang, []))\n",
    "        row.append(count)\n",
    "    intent_lang_matrix.append(row)\n",
    "\n",
    "# Convert to numpy array for plotting\n",
    "matrix = np.array(intent_lang_matrix)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(matrix, \n",
    "            annot=True, fmt='d', cmap='YlOrRd',\n",
    "            xticklabels=lang_names,\n",
    "            yticklabels=intent_names,\n",
    "            cbar_kws={'label': 'Number of Utterances'})\n",
    "plt.title('Intent Distribution Across 10 Languages\\n(Utterance Count per Intent-Language Pair)')\n",
    "plt.xlabel('Languages')\n",
    "plt.ylabel('Intents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print('\\nüìà Multilingual Intent Statistics:')\n",
    "print(f'  üéØ Total Unique Intents: {len(intent_groups)}')\n",
    "print(f'  üåç Languages Analyzed: {len(TOP_10_LANGUAGES)}')\n",
    "print(f'  üìù Total Utterances: {sum(intent_frequencies.values())}')\n",
    "print(f'  üìä Average Utterances per Intent: {sum(intent_frequencies.values())/len(intent_groups):.1f}')\n",
    "print(f'  üèÜ Most Common Intent: {ranked_intents[0][0]} ({ranked_intents[0][1]} utterances)')\n",
    "print(f'  üìâ Least Common Intent: {ranked_intents[-1][0]} ({ranked_intents[-1][1]} utterances)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll summarize our system's performance and discuss practical considerations for production deployment. The summary includes key performance metrics, cost analysis, and recommendations for scaling the system. Understanding these factors helps determine whether the current implementation meets your requirements and what modifications might be needed for production use. The cost analysis is particularly important since embedding generation requires API calls, and understanding the cost structure helps with budgeting and optimization decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('System summary\\n')\n",
    "\n",
    "print('Performance metrics:')\n",
    "if 'results' in locals():\n",
    "    print(f'Overall accuracy: {results[\"accuracy\"]*100:.2f}%')\n",
    "    print(f'Macro F1: {results[\"macro_f1\"]*100:.2f}%')\n",
    "    print(f'Weighted F1: {results[\"weighted_f1\"]*100:.2f}%')\n",
    "\n",
    "print(f'\\nmultilingual performance:')\n",
    "print(f'Same-language: {same_lang_acc*100:.2f}%')\n",
    "print(f'Multilingual: {cross_lang_acc*100:.2f}%')\n",
    "print(f'Transfer efficiency: {cross_lang_acc/same_lang_acc*100:.1f}%')\n",
    "\n",
    "print(f'\\nCost analysis:')\n",
    "print(f'Total API calls: {nova_embeddings.request_count}')\n",
    "print(f'Total cost: ${nova_embeddings.request_count * 0.0002:.4f}')\n",
    "print(f'Cost per sample: ${(nova_embeddings.request_count * 0.0002) / len(all_texts):.6f}')\n",
    "\n",
    "print(f'\\nSystem specifications:')\n",
    "print(f'Model: {NOVA_MODEL_ID}')\n",
    "print(f'Embedding dimension: {EMBEDDING_DIMENSION}d')\n",
    "print(f'Intent classes: {len(set(all_labels))}')\n",
    "print(f'Languages: {len(multilingual_data)}')\n",
    "print(f'Training samples: {len(all_texts)}')\n",
    "\n",
    "print('\\nProduction considerations:')\n",
    "print('Dataset expansion: Scale to full MASSIVE dataset for comprehensive coverage')\n",
    "print('Embedding caching: Store embeddings to reduce API calls for repeated inference')\n",
    "print('Batch processing: Process multiple requests together to improve throughput')\n",
    "print('Rate limiting: Implement proper rate limiting for Bedrock API calls')\n",
    "print('Architecture: Deploy with API Gateway and Lambda for real-time classification')\n",
    "print('Monitoring: Use CloudWatch for API usage and performance tracking')\n",
    "print('Optimization: Tune k-neighbors parameter based on dataset size')\n",
    "print('Confidence thresholds: Implement thresholds for uncertain predictions')\n",
    "\n",
    "print('\\nMultilingual intent classification system complete')\n",
    "print('The system demonstrates effective multilingual understanding through semantic embeddings,')\n",
    "print('providing a foundation for multilingual intent classification in production environments.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
