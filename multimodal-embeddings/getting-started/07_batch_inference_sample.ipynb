{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "batch-inference-intro",
   "metadata": {},
   "source": [
    "# Batch Inference with Nova Embeddings\n",
    "\n",
    "This notebook demonstrates how to prepare input data for batch inference using the Nova Embeddings model. We'll use 100 text files to create a JSONL file for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restore-variables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore variables from setup notebook\n",
    "%store -r s3_bucket\n",
    "print(f\"Using S3 bucket: {s3_bucket}\")\n",
    "%store -r region_name\n",
    "print(f\"Using region: {region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-images",
   "metadata": {},
   "source": [
    "## Step 1: Download and Prepare Text Dataset\n",
    "\n",
    "We'll download a public text dataset and create individual text files for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-to-s3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Load OpenAI HumanEval dataset\n",
    "ds = load_dataset(\"openai/openai_humaneval\")\n",
    "test_data = ds['test']\n",
    "\n",
    "# Create text files from first 100 rows\n",
    "text_dir = Path('text_dataset')\n",
    "text_dir.mkdir(exist_ok=True)\n",
    "\n",
    "text_files = []\n",
    "for i in range(min(100, len(test_data))):\n",
    "    row = test_data[i]\n",
    "    # Combine prompt and canonical_solution for richer text content\n",
    "    text_content = f\"{row['prompt']}\\n\\n# Solution:\\n{row['canonical_solution']}\"\n",
    "    \n",
    "    filename = f\"text_{i+1:03d}.txt\"\n",
    "    filepath = text_dir / filename\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(text_content)\n",
    "    text_files.append((filename, str(filepath)))\n",
    "\n",
    "print(f\"Created {len(text_files)} text files in {text_dir}\")\n",
    "print(f\"Sample files: {[f[0] for f in text_files[:5]]}\")\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client('s3', region_name=region_name)\n",
    "s3_prefix = 'batch-inference/'\n",
    "\n",
    "uploaded_texts = []\n",
    "\n",
    "for filename, filepath in text_files:\n",
    "    s3_key = f\"{s3_prefix}{filename}\"\n",
    "    s3_uri = f\"s3://{s3_bucket}/{s3_key}\"\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(filepath, s3_bucket, s3_key)\n",
    "        uploaded_texts.append((filename, s3_uri))\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {filename}: {e}\")\n",
    "\n",
    "print(f\"\\nUploaded {len(uploaded_texts)} text files to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-jsonl",
   "metadata": {},
   "source": [
    "## Step 2: Prepare JSONL Input File\n",
    "\n",
    "Create the JSONL file with the required format for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-jsonl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AWS account ID for bucketOwner parameter\n",
    "sts_client = boto3.client('sts', region_name=region_name)\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "jsonl_records = []\n",
    "\n",
    "for i, (filename, s3_uri) in enumerate(uploaded_texts, 1):\n",
    "    print(s3_uri)\n",
    "    record = {\n",
    "        \"recordId\": f\"record{i:03d}\",\n",
    "        \"modelInput\": {\n",
    "            \"taskType\": \"SINGLE_EMBEDDING\",\n",
    "            \"singleEmbeddingParams\": {\n",
    "                \"embeddingPurpose\": \"GENERIC_INDEX\",\n",
    "                \"embeddingDimension\": 3072,\n",
    "                \"text\": {\n",
    "                    \"source\": {\n",
    "                        \"s3Location\": {\n",
    "                            \"uri\": s3_uri,\n",
    "                            \"bucketOwner\": account_id\n",
    "                        }\n",
    "                    },\n",
    "                    \"truncationMode\": \"END\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    jsonl_records.append(record)\n",
    "\n",
    "# Write JSONL file\n",
    "jsonl_filename = 'batch_inference_input.jsonl'\n",
    "with open(jsonl_filename, 'w') as f:\n",
    "    for record in jsonl_records:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"Created {jsonl_filename} with {len(jsonl_records)} records\")\n",
    "print(f\"\\nFirst record example:\")\n",
    "print(json.dumps(jsonl_records[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-jsonl",
   "metadata": {},
   "source": [
    "## Step 3: Upload JSONL File to S3\n",
    "\n",
    "Upload the input file to S3 for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-jsonl-to-s3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_s3_uri = f\"s3://{s3_bucket}/batch-inference/\"\n",
    "input_s3_key = f\"batch-inference/{jsonl_filename}\"\n",
    "\n",
    "s3_client.upload_file(jsonl_filename, s3_bucket, input_s3_key)\n",
    "print(f\"Uploaded input file to: {input_s3_key}\")\n",
    "\n",
    "# Also define output location\n",
    "output_s3_uri = f\"s3://{s3_bucket}/batch-inference/output/\"\n",
    "print(f\"Output will be written to: {output_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-iam-role",
   "metadata": {},
   "source": [
    "## Step 4: Create IAM Role for Batch Inference\n",
    "\n",
    "Create the required IAM role with permissions for Bedrock batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-iam-role",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "iam_client = boto3.client('iam', region_name=region_name)\n",
    "role_name = 'BedrockBatchExecutionRole'\n",
    "bedrock_role_arn = f\"arn:aws:iam::{account_id}:role/{role_name}\"\n",
    "\n",
    "# Trust policy for Bedrock service\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Permissions policy\n",
    "permissions_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:InvokeModel\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": f\"arn:aws:s3:::{s3_bucket}/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": f\"arn:aws:s3:::{s3_bucket}\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Check if role exists\n",
    "    iam_client.get_role(RoleName=role_name)\n",
    "    print(f\"Role {role_name} already exists\")\n",
    "except iam_client.exceptions.NoSuchEntityException:\n",
    "    # Create role\n",
    "    iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "    \n",
    "    # Attach inline policy\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName='BedrockBatchExecutionPolicy',\n",
    "        PolicyDocument=json.dumps(permissions_policy)\n",
    "    )\n",
    "    \n",
    "    print(f\"Created role: {bedrock_role_arn}\")\n",
    "\n",
    "print(f\"Using role: {bedrock_role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start-batch-job",
   "metadata": {},
   "source": [
    "## Step 5: Start Batch Inference Job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-batch-job",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create job name with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "job_name = f\"nova-embeddings-batch-{timestamp}\"\n",
    "\n",
    "print(f\"Job name: {job_name}\")\n",
    "print(f\"Role ARN: {bedrock_role_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-batch-job",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock', region_name=region_name)\n",
    "\n",
    "# Prepare job payload\n",
    "job_payload = {\n",
    "    'jobName': job_name,\n",
    "    'roleArn': bedrock_role_arn,\n",
    "    'modelId': 'amazon.nova-2-multimodal-embeddings-v1:0',\n",
    "    'inputDataConfig': {\n",
    "        's3InputDataConfig': {\n",
    "            's3InputFormat': 'JSONL',\n",
    "            's3Uri': input_s3_uri\n",
    "        }\n",
    "    },\n",
    "    'outputDataConfig': {\n",
    "        's3OutputDataConfig': {\n",
    "            's3Uri': output_s3_uri\n",
    "        }\n",
    "    },\n",
    "    'timeoutDurationInHours': 24\n",
    "}\n",
    "\n",
    "print(\"Job payload:\")\n",
    "for key, value in job_payload.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "start-job",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Start the batch inference job\n",
    "    response = bedrock_client.create_model_invocation_job(**job_payload)\n",
    "    \n",
    "    print(f\"Request ID: {response.get(\"ResponseMetadata\").get(\"RequestId\")}\") \n",
    "    print(f\"Job ARN: {response.get(\"jobArn\")}\")\n",
    "    \n",
    "    # Store job ARN for later use\n",
    "    job_arn = response['jobArn']\n",
    "    %store job_arn\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to start batch job: {e}\")\n",
    "    print(\"\\nCommon issues:\")\n",
    "    print(\"1. IAM role doesn't exist or lacks permissions\")\n",
    "    print(\"2. S3 bucket/objects are not accessible\")\n",
    "    print(\"3. Model ID is incorrect or not enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor-job",
   "metadata": {},
   "source": [
    "## Step 6: Monitor Job Status\n",
    "\n",
    "Check the status of your batch inference job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-job-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check job details\n",
    "try:\n",
    "    job_details = bedrock_client.get_model_invocation_job(jobIdentifier=job_arn)\n",
    "    \n",
    "    # Convert datetime objects to strings for JSON serialization\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def json_serial(obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "    \n",
    "    print(json.dumps(job_details, indent=2, default=json_serial))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error checking job status: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download-results",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "Download the batch inference results when the job is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-results-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if job is completed and download results\n",
    "try:\n",
    "    job_details = bedrock_client.get_model_invocation_job(jobIdentifier=job_arn)\n",
    "    \n",
    "    if job_details['status'] == 'Completed':\n",
    "        print(\"Job completed! Downloading results...\")\n",
    "        \n",
    "        # Create results directory\n",
    "        results_dir = Path('batch_results')\n",
    "        results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Extract job ID from job ARN\n",
    "        job_id = job_details['jobArn'].split('/')[-1]\n",
    "        \n",
    "        # List objects in the output S3 location with job ID\n",
    "        output_prefix = f'batch-inference/output/{job_id}'\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket,\n",
    "            Prefix=output_prefix\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                s3_key = obj['Key']\n",
    "                filename = Path(s3_key).name\n",
    "                local_path = results_dir / filename\n",
    "                \n",
    "                # Download the file\n",
    "                s3_client.download_file(s3_bucket, s3_key, str(local_path))\n",
    "                print(f\"Downloaded: {filename}\")\n",
    "                \n",
    "                # If it's a JSONL file, show sample results\n",
    "                if filename.endswith('.jsonl'):\n",
    "                    with open(local_path, 'r') as f:\n",
    "                        lines = f.readlines()[:3]  # Show first 3 results\n",
    "                        print(f\"\\nSample results from {filename}:\")\n",
    "                        for i, line in enumerate(lines, 1):\n",
    "                            result = json.loads(line)\n",
    "                            print(f\"Record {i}: {result['recordId']} - Status: {result.get('modelOutput', {}).get('status', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"No output files found\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Job status: {job_details['status']} - Results not ready yet\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33199ed-91a0-4e03-9ddf-46dfaac0bd00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
