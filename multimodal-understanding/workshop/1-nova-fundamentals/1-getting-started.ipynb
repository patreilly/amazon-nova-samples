{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Getting Started with Amazon Nova Models\n",
    "\n",
    "Amazon Nova foundation models deliver frontier intelligence and industry leading price-performance. Through Amazon Bedrock, organizations can seamlessly build and scale generative AI applications with Nova that are safe, reliable, and cost-effective.\n",
    "\n",
    "This workshop will focus primarily on **Amazon Nova Understanding Models:**\n",
    "\n",
    "**Amazon Nova Micro:** Lightening fast, cost-effective text-only model\n",
    "\n",
    "**Amazon Nova Lite:** Fastest, most affordable multimodal FM in the industry for its intelligence tier\n",
    "\n",
    "**Amazon Nova Pro:** The fastest, most cost-effective, state-of-the-art multimodal model in the industry\n",
    "\n",
    "**Amazon Nova Premier:** Most capable multimodal model for complex tasks and the best teacher for distilling custom models for cost-effective applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f580e-366c-498f-96ec-7a56908660d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:04:37.180942Z",
     "iopub.status.busy": "2025-07-25T19:04:37.180391Z",
     "iopub.status.idle": "2025-07-25T19:04:37.185777Z",
     "shell.execute_reply": "2025-07-25T19:04:37.184729Z",
     "shell.execute_reply.started": "2025-07-25T19:04:37.180912Z"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    " <b>You will see pip dependency errors, you can safely ignore these errors.</b>\n",
    "    \n",
    "    IGNORE ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Using the boto3 SDK in Python\n",
    "\n",
    "Interaction with the Bedrock API is done via the AWS SDK for Python: [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html).\n",
    "\n",
    "### Using the Default Credential Chain\n",
    "\n",
    "If you are running this notebook from [Amazon SageMaker Studio](https://aws.amazon.com/sagemaker/studio/) and your SageMaker Studio [execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) has permissions to access Bedrock, you can just run the cells below as-is. This is also the case if you are running these notebooks from a computer whose default AWS credentials have access to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from IPython.display import display, Markdown, Video\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import json\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "For hosted workshop, we are accessing Nova models from us-west-2 region via CRIS. For more information, check out the [inference profiles documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the region from the SageMaker session\n",
    "region_name = sagemaker.Session().boto_region_name\n",
    "print(f\"Current AWS Region: {region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MICRO_MODEL_ID = \"us.amazon.nova-micro-v1:0\"\n",
    "LITE_MODEL_ID = \"us.amazon.nova-lite-v1:0\"\n",
    "PRO_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "PREMIER_MODEL_ID = \"us.amazon.nova-premier-v1:0\"\n",
    "\n",
    "%store MICRO_MODEL_ID\n",
    "%store LITE_MODEL_ID\n",
    "%store PRO_MODEL_ID\n",
    "%store PREMIER_MODEL_ID\n",
    "%store region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Validate the Connection\n",
    "\n",
    "We can check that the client works by trying out the `list_foundation_models()` method, which will tell us all the models available for us to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"bedrock\")\n",
    "[\n",
    "    model[\"modelId\"]\n",
    "    for model in client.list_foundation_models()[\"modelSummaries\"]\n",
    "    if model[\"modelId\"].startswith(\"amazon.nova\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### InvokeModel Request and Response Format\n",
    "\n",
    "The `invoke_model()` method of the Amazon Bedrock runtime client (InvokeModel API) will be the primary method we use for most of our Text Generation and Processing tasks.\n",
    "\n",
    "Although the method is shared, the format of input and output varies depending on the foundation model used. The sample JSON schema below shows the structure for Amazon Nova models:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"system\": [\n",
    "    {\n",
    "      \"text\": string\n",
    "    }\n",
    "  ],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\", // first turn should always be the user turn\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": string\n",
    "        },\n",
    "        {\n",
    "          \"image\": {\n",
    "            \"format\": \"jpeg\"| \"png\" | \"gif\" | \"webp\",\n",
    "            \"source\": {\n",
    "              \"bytes\": \"base64EncodedImageDataHere...\" // base64-encoded binary\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"video\": {\n",
    "            \"format\": \"mkv\" | \"mov\" | \"mp4\" | \"webm\" | \"three_gp\" | \"flv\" | \"mpeg\" | \"mpg\" | \"wmv\",\n",
    "            \"source\": {\n",
    "            // source can be s3 location of base64 bytes based on size of input file. \n",
    "               \"s3Location\": {\n",
    "                \"uri\": string, // example: s3://my-bucket/object-key\n",
    "                \"bucketOwner\": string // (Optional) example: 123456789012)\n",
    "               }\n",
    "              \"bytes\": \"base64EncodedImageDataHere...\" // base64-encoded binary\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": string // prefilling assistant turn\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    " \"inferenceConfig\":{ // all Optional\n",
    "    \"max_new_tokens\": int, // greater than 0, equal or less than 5k (default: dynamic*)\n",
    "    \"temperature\": float, // greater then 0 and less than 1.0 (default: 0.7)\n",
    "    \"top_p\": float, // greater than 0, equal or less than 1.0 (default: 0.9)\n",
    "    \"top_k\": int // 0 or greater (default: 50)\n",
    "    \"stopSequences\": [string]\n",
    "  },\n",
    "  \"toolConfig\": { // all Optional\n",
    "        \"tools\": [\n",
    "                {\n",
    "                    \"toolSpec\": {\n",
    "                        \"name\": string // meaningful tool name (Max char: 64)\n",
    "                        \"description\": string // meaningful description of the tool\n",
    "                        \"inputSchema\": {\n",
    "                            \"json\": { // The JSON schema for the tool\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    <args>: { // arguments \n",
    "                                        \"type\": string, // argument data type\n",
    "                                        \"description\": string // meaningful description\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\n",
    "                                    string // args\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "        \"toolChoice\": \"auto\" // Three supported parameter options: tool, any, and auto\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Required and Optional Parameters\n",
    "\n",
    "* `system` – (Optional) The system prompt for the request.\n",
    "  * A system prompt provides context and instructions to Amazon Nova, such as specifying a particular goal or role.\n",
    "\n",
    "* `messages` – (Required) The input messages.\n",
    "  * `role` – The role of the conversation turn. Valid values are user and assistant. \n",
    "  * `content` – (required) The content of the conversation turn.\n",
    "    * `type` – (required) The type of the content. Valid values are image, text, video\n",
    "      * if chosen text (text content)\n",
    "        * `text` - The content of the conversation turn. \n",
    "      * If chosen Image (image content)\n",
    "        * `source` – (required) The base64 encoded image bytes for the image.\n",
    "        * `format` – (required) The type of the image (jpeg, png, webp, gif)\n",
    "      * If chosen video: (video content)\n",
    "        * `source` – (required) The base64 encoded video bytes or S3 URI with bucket owner\n",
    "        * `format` – (required) The type of the video (mkv, mov, mp4, webm, etc.)\n",
    "\n",
    "* `inferenceConfig` – (Optional) Inference configuration parameters\n",
    "  * `max_new_tokens` – Maximum number of tokens to generate (max 5K)\n",
    "  * `temperature` – Amount of randomness in the response\n",
    "  * `top_p` – Nucleus sampling probability threshold\n",
    "  * `top_k` – Limiting sampling to top K options for each token\n",
    "  * `stopSequences` – Array of strings to stop generation when encountered\n",
    "\n",
    "* `toolConfig` – (Optional) JSON object containing the tool specification and tool choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Text Understanding with Nova Models\n",
    "\n",
    "Note: The examples below can work with Nova Micro, Nova Lite, and Nova Pro models. We're using Nova Micro for illustrative purposes, but you can substitute any model of the Nova family.\n",
    "\n",
    "### Synchronous API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_nova(\n",
    "    model,\n",
    "    messages,\n",
    "    system_message=\"\",\n",
    "    streaming=False,\n",
    "    max_tokens=1024,\n",
    "    temp=0.7,\n",
    "    top_p=0.99,\n",
    "    top_k=20,\n",
    "    tools=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Call Amazon Nova models with various parameters.\n",
    "    \n",
    "    Args:\n",
    "        model (str): The model ID to use\n",
    "        messages (list): List of message objects with role and content\n",
    "        system_message (str, optional): System prompt. Defaults to \"\".\n",
    "        streaming (bool, optional): Whether to use streaming API. Defaults to False.\n",
    "        max_tokens (int, optional): Maximum tokens to generate. Defaults to 512.\n",
    "        temp (float, optional): Temperature parameter. Defaults to 0.7.\n",
    "        top_p (float, optional): Top-p parameter. Defaults to 0.99.\n",
    "        top_k (int, optional): Top-k parameter. Defaults to 20.\n",
    "        tools (list, optional): List of tool specifications. Defaults to None.\n",
    "        verbose (bool, optional): Whether to print request body. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        tuple or stream: Model response and content text if not streaming, else stream\n",
    "    \"\"\"\n",
    "    client = boto3.client(\"bedrock-runtime\")\n",
    "    \n",
    "    # Prepare system prompt\n",
    "    system_list = [{\"text\": system_message}]\n",
    "    \n",
    "    # Prepare inference parameters\n",
    "    inf_params = {\n",
    "        \"max_new_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"temperature\": temp,\n",
    "    }\n",
    "    \n",
    "    # Build request body\n",
    "    request_body = {\n",
    "        \"messages\": messages,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "    \n",
    "    # Add tool configuration if provided\n",
    "    if tools is not None:\n",
    "        tool_config = []\n",
    "        for tool in tools:\n",
    "            tool_config.append({\"toolSpec\": tool})\n",
    "        request_body[\"toolConfig\"] = {\"tools\": tool_config}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Request Body\", request_body)\n",
    "    \n",
    "    if not streaming:\n",
    "        # Use synchronous API\n",
    "        response = client.invoke_model(modelId=model, body=json.dumps(request_body))\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        return model_response, model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    else:\n",
    "        # Use streaming API\n",
    "        response = client.invoke_model_with_response_stream(\n",
    "            modelId=model, body=json.dumps(request_body)\n",
    "        )\n",
    "        return response[\"body\"]\n",
    "\n",
    "\n",
    "def get_base64_encoded_value(media_path):\n",
    "    \"\"\"Convert media file to base64 encoded string.\n",
    "    \n",
    "    Args:\n",
    "        media_path (str): Path to the media file\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 encoded string\n",
    "    \"\"\"\n",
    "    with open(media_path, \"rb\") as media_file:\n",
    "        binary_data = media_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "        return base64_string\n",
    "\n",
    "\n",
    "def print_output(content_text):\n",
    "    \"\"\"Display model output as Markdown.\n",
    "    \n",
    "    Args:\n",
    "        content_text (str): Text to display\n",
    "    \"\"\"\n",
    "    display(Markdown(content_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Use Amazon Nova with a system prompt\n",
    "\n",
    "System prompts provide context and instructions to guide the model's behavior. Here we demonstrate how to use a system prompt to instruct the model to act as a creative writing assistant for marketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "system_message = \"Act as a telecom industry marketer for AnyCompany Telecom. When the user provides you with a topic, write a LinkedIn Launch Post about that topic.\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \"Meet our AI-powered customer support assistant\"}\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "model_response, content_text = call_nova(\n",
    "    MICRO_MODEL_ID, messages, system_message=system_message\n",
    ")\n",
    "\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print_output(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d98c1-0b15-4fd1-90e2-631644b0e1c6",
   "metadata": {},
   "source": [
    "**Note:** Review Nova's output for the task above. Do you see anything that you may not want in a public message for a business? You can optimize the prompt to address this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58321d8f-1a3b-4d0a-bf0b-4ee9e30b5ee5",
   "metadata": {},
   "source": [
    "\n",
    "### Multi-lingual content generation\n",
    "\n",
    "For this context, imagine you are a customer communications associate who communicates incidents to global users. You use Amazon Nova to help you generate messages in multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc29fa4-a9b1-4c61-832e-c59bc2a42fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a customer communications specialist who balances technical detail with clarity.\"\n",
    "incident_context = \"\"\"\n",
    "Event: System Maintenance Impact\n",
    "Time: 15-minute delay in customer portal\n",
    "Cause: Database optimization\n",
    "Status: Completed\n",
    "Benefit: 50% faster loading times\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": f\"Generate brief customer emails in English, Spanish, French, and Japanese about this maintenance impact - be clear but friendly:\\n\\n{incident_context}\"}\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "model_response, content_text = call_nova(\n",
    "    MICRO_MODEL_ID, messages, system_message=system_message\n",
    ")\n",
    "\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print_output(content_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cfa872b-edbe-459b-ad3a-275813572d1b",
   "metadata": {},
   "source": [
    "\n",
    "## Multimodal Understanding with Nova Models\n",
    "\n",
    "Let's show how Amazon Nova can analyze multimodal content (e.g. an image). The following example demonstrates how to pass images to Nova models for multimodal understanding. Note that multimodal capabilities are only available with Nova Lite, Nova Pro models and Nova Premier (not with Nova Micro)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Image Understanding\n",
    "\n",
    "Let's see how Amazon Nova models perform on image understanding\n",
    "\n",
    "Amazon Nova models allow you to include multiple images in a payload with a maximum size limit of 25MB. The model can:\n",
    "- Analyze images and answer questions about them\n",
    "- Classify images\n",
    "- Summarize image content based on provided instructions\n",
    "\n",
    "In this example, we'll pass an image of a coverage map and give Nova a task\n",
    "\n",
    "![Coverage map](../1-nova-fundamentals/images/global_coverage_map.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a coverage expert explaining service availability with high accuracy\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"source\": {\n",
    "                        \"bytes\": get_base64_encoded_value(\n",
    "                            \"../1-nova-fundamentals/images/global_coverage_map.png\"\n",
    "                        )\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"What does this coverage map tell us about service quality in different continents? List the continents and explanation in bullet points\"\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "model_response, content_text = call_nova(\n",
    "    LITE_MODEL_ID, messages, system_message=system_message, max_tokens=300\n",
    ")\n",
    "\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print_output(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6527867-395b-4119-a704-c0c64627d976",
   "metadata": {},
   "source": [
    "### Video Understanding\n",
    "\n",
    "The Amazon Nova models allow you to include a single video in the payload, which can be provided either in base64 format or through an Amazon S3 URI. When using the base64 method, the overall payload size must remain within 25MB. However, you can specify an Amazon S3 URI for video understanding. This approach enables you to leverage the model for longer videos without being constrained by the overall payload size limitation. Amazon Nova models can analyze the passed video and answer questions, classify a video, and summarize information in the video based on provided instructions.\n",
    "\n",
    "Let's analyze a video using Amazon Nova. Here, we will use base64 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c3f51-a9c2-42f9-99a6-e44df81b2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the video?\n",
    "video_path = \"video/cell-phone.mp4\"\n",
    "\n",
    "# Verify file exists\n",
    "if os.path.exists(video_path):\n",
    "    # Display video with controls and specified dimensions\n",
    "    display(Video(video_path, \n",
    "                 embed=True, \n",
    "                 width=800,  # Adjust width as needed\n",
    "                 height=450, \n",
    "                 html_attributes=\"controls autoplay loop\"))\n",
    "else:\n",
    "    print(f\"Error: Video file not found at {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d68715-7284-4c04-9f1c-291def289cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an expert product merchandiser.\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\n",
    "                        \"bytes\": get_base64_encoded_value(\n",
    "                            video_path\n",
    "                        )\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Brainstorm 3 ideas on how to place this creative asset for marketing\"\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "model_response, content_text = call_nova(\n",
    "    LITE_MODEL_ID, messages, system_message=system_message, max_tokens=300\n",
    ")\n",
    "\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print_output(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rcuf0ndl8g9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we explored the foundational capabilities of Amazon Nova models:\n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "1. **Understanding Amazon Nova Model Family**\n",
    "   - **Text Understanding**: Amazon Nova Micro, Lite, Pro, and Premier offer varying levels of capabilities with different price-performance ratios\n",
    "   - **Multimodal Processing**: Nova Lite and Pro can process text, images, and videos\n",
    "\n",
    "2. **Core Capabilities**\n",
    "   - **Text Processing**: Demonstrated synchronous and streaming API calls\n",
    "   - **System Prompts**: Used system prompts to guide model behavior\n",
    "   - **Multimodal Understanding**: Analyzed image and video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa639f3-7fd4-4258-a8be-b3d8ab48b70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
